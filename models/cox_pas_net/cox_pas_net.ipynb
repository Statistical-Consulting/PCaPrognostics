{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Setup directories\n",
    "RESULTS_DIR = os.path.join(os.getcwd(), 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Imports\n",
    "from preprocessing.data_container import DataContainer\n",
    "from utils.evaluation import cindex_score\n",
    "from models.modelling_process import ModellingProcess\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from utils.evaluation import EarlyStoppingMonitor\n",
    "from models.cox_pas_net_model import Cox_PASNet_Model\n",
    "import torch\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CONFIG = {\n",
    "    'use_pca': False,\n",
    "    'pca_threshold': 0.95,\n",
    "    'gene_type': 'intersect',\n",
    "    'use_imputed': True,\n",
    "    'use_cohorts': False,\n",
    "    'select_random': False, \n",
    "    'clinical_covs' : ['AGE']\n",
    "}\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "''' Net Settings'''\n",
    "In_Nodes = 13214 ###number of genes\n",
    "Pathway_Nodes = 143 ###number of pathways\n",
    "Hidden_Nodes = 100 ###number of hidden nodes\n",
    "Out_Nodes = 10 ###number of hidden nodes in the last hidden layer\n",
    "''' Initialize '''\n",
    "Learning_Rate = 0.01\n",
    "L2 = 0\n",
    "num_epochs = 3000 ###for grid search\n",
    "Num_EPOCHS = 15 ###for training\n",
    "###sub-network setup\n",
    "Dropout_Rate = [0.1, 0.1]\n",
    "''' load data and pathway '''\n",
    "pathway_mask = pd.read_csv(\"../../pathway_mask.csv\", index_col = 0)\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv': {\n",
    "        'model__Learning_Rate': [0.01],\n",
    "        'model__L2': [0], \n",
    "        'model__Num_EPOCHS': [1]\n",
    "        },\n",
    "    'refit': False, \n",
    "    'do_nested_resampling': True, \n",
    "    'path' : RESULTS_DIR, \n",
    "    'fname_cv' : 'results_intersect'}\n",
    "\n",
    "pipeline_steps = [('model', Cox_PASNet_Model(pathway_mask=pathway_mask, clin_covs=['AGE']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = ModellingProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 18:13:52,613 - INFO - Loading data...\n",
      "2025-01-03 18:14:03,481 - INFO - Found clinical data specification\n",
      "2025-01-03 18:14:03,521 - INFO - Loaded data: 1091 samples, 13215 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1091 entries, Atlanta_2014_Long.PT081 to Stockholm_2016_Ross_Adams.STKHLM9246\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   AGE                1091 non-null   float64\n",
      " 1   TISSUE             1091 non-null   object \n",
      " 2   CLIN_T_STAGE       1091 non-null   object \n",
      " 3   PATH_T_STAGE       1091 non-null   object \n",
      " 4   GLEASON_SCORE      1091 non-null   int64  \n",
      " 5   PRE_OPERATIVE_PSA  1091 non-null   float64\n",
      " 6   MONTH_TO_BCR       1091 non-null   float64\n",
      " 7   BCR_STATUS         1091 non-null   int64  \n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 76.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mp.prepare_data(DATA_CONFIG, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 18:18:12,247 - INFO - No additional monitoring detected\n",
      "2025-01-03 18:18:12,248 - INFO - Start model training...\n",
      "2025-01-03 18:18:12,248 - INFO - Input data shape: X=(1091, 13215)\n",
      "2025-01-03 18:18:12,249 - INFO - Nested resampling...\n",
      "2025-01-03 18:18:12,250 - INFO - Starting nested resampling...\n",
      "2025-01-03 18:18:12,252 - INFO - Data shape: X=(1091, 13215), groups=9 unique\n",
      "2025-01-03 18:18:12,255 - INFO - \n",
      "Outer fold 1\n",
      "2025-01-03 18:18:12,307 - INFO - Test cohort: Atlanta_2014_Long\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_modelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\models\\modelling_process.py:85\u001b[0m, in \u001b[0;36mModellingProcess.do_modelling\u001b[1;34m(self, pipeline_steps, config)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_nested_resampling: \n\u001b[0;32m     84\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNested resampling...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrs \u001b[38;5;241m=\u001b[39m \u001b[43mnested_resampling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter_cv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname_cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_results(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfname_cv, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, cv_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrs, pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\utils\\resampling.py:66\u001b[0m, in \u001b[0;36mnested_resampling\u001b[1;34m(estimator, X, y, groups, param_grid, monitor, ss, outer_cv, inner_cv, scoring)\u001b[0m\n\u001b[0;32m     62\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of iterations early stopping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_results\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mn_estimators_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m---> 66\u001b[0m     inner_results \u001b[38;5;241m=\u001b[39m \u001b[43minner_gcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m inner_cv_results \u001b[38;5;241m=\u001b[39m inner_results\u001b[38;5;241m.\u001b[39mcv_results_\n\u001b[0;32m     69\u001b[0m inner_best_params \u001b[38;5;241m=\u001b[39m inner_results\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laeti\\PCaPrognostics\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mp.do_modelling(pipeline_steps, MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import torch\n",
    "# from lifelines.utils import concordance_index\n",
    "# from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "# import logging\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #from Model import Cox_PASmodel\n",
    "# #from Subself.modelwork_SparseCoding import dropout_mask, s_mask\n",
    "# import torch \n",
    "# import numpy as np\n",
    "# import math\n",
    "\n",
    "# def dropout_mask(n_node, drop_p):\n",
    "# \t'''Construct a binary matrix to randomly drop nodes in a layer.\n",
    "# \tInput:\n",
    "# \t\tn_node: number of nodes in the layer.\n",
    "# \t\tdrop_p: the probability that a node is to be dropped.\n",
    "# \tOutput:\n",
    "# \t\tmask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.\n",
    "# \t'''\n",
    "# \tkeep_p = 1.0 - drop_p\n",
    "# \tmask = torch.Tensor(np.random.binomial(1, keep_p, size=n_node))\n",
    "# \t###if gpu is being used\n",
    "# \tif torch.cuda.is_available():\n",
    "# \t\tmask = mask.cuda()\n",
    "# \t###\n",
    "# \treturn mask\n",
    "\n",
    "# def s_mask(sparse_level, param_matrix, nonzero_param_1D, dtype):\n",
    "# \t'''Construct a binary matrix w.r.t. a sparsity level of weights between two consecutive layers\n",
    "# \tInput:\n",
    "# \t\tsparse_level: a percentage value in [0, 100) represents the proportion of weights in a sub-network to be dropped.\n",
    "# \t\tparam_matrix: a weight matrix for entrie network.\n",
    "# \t\tnonzero_param_1D: 1D of non-zero 'param_matrix' (which is the weights selected from a sub-network).\n",
    "# \t\tdtype: define the data type of tensor (i.e. dtype=torch.FloatTensor).\n",
    "# \tOutput:\n",
    "# \t\tparam_mask: a binary matrix, where 1 --> keep the node; 0 --> drop the node.\n",
    "# \t'''\n",
    "# \t###take the absolute values of param_1D \n",
    "# \tnon_neg_param_1D = torch.abs(nonzero_param_1D)\n",
    "# \t###obtain the number of params\n",
    "# \tnum_param = nonzero_param_1D.size(0)\n",
    "# \t###obtain the kth number based on sparse_level\n",
    "# \ttop_k = math.ceil(num_param*(100-sparse_level)*0.01)\n",
    "# \t###obtain the k largest params\n",
    "# \tsorted_non_neg_param_1D, indices = torch.topk(non_neg_param_1D, top_k)\n",
    "# \tparam_mask = torch.abs(param_matrix) > sorted_non_neg_param_1D.min()\n",
    "# \tparam_mask = param_mask.type(dtype)\n",
    "# \t###if gpu is being used\n",
    "# \tif torch.cuda.is_available():\n",
    "# \t\tparam_mask = param_mask.cuda()\n",
    "# \t###\n",
    "# \treturn param_mask\n",
    "\n",
    "\n",
    "\n",
    "# #from Survival_CostFunc_CIndex import R_set, neg_par_log_likelihood, c_index\n",
    "# import torch\n",
    "\n",
    "# def R_set(x):\n",
    "# \t'''Create an indicator matrix of risk sets, where T_j >= T_i.\n",
    "# \tNote that the input data have been sorted in descending order.\n",
    "# \tInput:\n",
    "# \t\tx: a PyTorch tensor that the number of rows is equal to the number of samples.\n",
    "# \tOutput:\n",
    "# \t\tindicator_matrix: an indicator matrix (which is a lower traiangular portions of matrix).\n",
    "# \t'''\n",
    "# \tn_sample = x.size(0)\n",
    "# \tmatrix_ones = torch.ones(n_sample, n_sample)\n",
    "# \tindicator_matrix = torch.tril(matrix_ones)\n",
    "\n",
    "# \treturn(indicator_matrix)\n",
    "\n",
    "\n",
    "# def neg_par_log_likelihood(pred, ytime, yevent):\n",
    "# \t'''Calculate the average Cox negative partial log-likelihood.\n",
    "# \tNote that this function requires the input data have been sorted in descending order.\n",
    "# \tInput:\n",
    "# \t\tpred: linear predictors from trained model.\n",
    "# \t\tytime: true survival time from load_data().\n",
    "# \t\tyevent: true censoring status from load_data().\n",
    "# \tOutput:\n",
    "# \t\tcost: the cost that is to be minimized.\n",
    "# \t'''\n",
    "# \tn_observed = yevent.sum(0)\n",
    "# \tytime_indicator = R_set(ytime)\n",
    "# \t###if gpu is being used\n",
    "# \tif torch.cuda.is_available():\n",
    "# \t\tytime_indicator = ytime_indicator.cuda()\n",
    "# \t###\n",
    "# \trisk_set_sum = ytime_indicator.mm(torch.exp(pred)) \n",
    "# \tdiff = pred - torch.log(risk_set_sum)\n",
    "# \tsum_diff_in_observed = torch.transpose(diff, 0, 1).mm(yevent)\n",
    "# \tcost = (- (sum_diff_in_observed / n_observed)).reshape((-1,))\n",
    "\n",
    "# \treturn(cost)\n",
    "\n",
    "\n",
    "# def c_index(pred, ytime, yevent):\n",
    "# \t'''Calculate concordance index to evaluate models.\n",
    "# \tInput:\n",
    "# \t\tpred: linear predictors from trained model.\n",
    "# \t\tytime: true survival time from load_data().\n",
    "# \t\tyevent: true censoring status from load_data().\n",
    "# \tOutput:\n",
    "# \t\tconcordance_index: c-index (between 0 and 1).\n",
    "# \t'''\n",
    "# \tn_sample = len(ytime)\n",
    "# \tytime_indicator = R_set(ytime)\n",
    "# \tytime_matrix = ytime_indicator - torch.diag(torch.diag(ytime_indicator))\n",
    "# \t###T_i is uncensored\n",
    "# \tcensor_idx = (yevent == 0).nonzero()\n",
    "# \tzeros = torch.zeros(n_sample)\n",
    "# \tytime_matrix[censor_idx, :] = zeros\n",
    "# \t###1 if pred_i < pred_j; 0.5 if pred_i = pred_j\n",
    "# \tpred_matrix = torch.zeros_like(ytime_matrix)\n",
    "# \tfor j in range(n_sample):\n",
    "# \t\tfor i in range(n_sample):\n",
    "# \t\t\tif pred[i] < pred[j]:\n",
    "# \t\t\t\tpred_matrix[j, i]  = 1\n",
    "# \t\t\telif pred[i] == pred[j]: \n",
    "# \t\t\t\tpred_matrix[j, i] = 0.5\n",
    "\t\n",
    "# \tconcord_matrix = pred_matrix.mul(ytime_matrix)\n",
    "# \t###numerator\n",
    "# \tconcord = torch.sum(concord_matrix)\n",
    "# \t###denominator\n",
    "# \tepsilon = torch.sum(ytime_matrix)\n",
    "# \t###c-index = numerator/denominator\n",
    "# \tconcordance_index = torch.div(concord, epsilon)\n",
    "# \t###if gpu is being used\n",
    "# \tif torch.cuda.is_available():\n",
    "# \t\tconcordance_index = concordance_index.cuda()\n",
    "# \t###\n",
    "# \treturn(concordance_index)\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import copy\n",
    "# from scipy.interpolate import interp1d\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# dtype = torch.FloatTensor\n",
    "\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class Cox_PASNet(nn.Module):\n",
    "# \tdef __init__(self, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, Pathway_Mask):\n",
    "# \t\tsuper(Cox_PASNet, self).__init__()\n",
    "# \t\tself.tanh = nn.Tanh()\n",
    "# \t\tself.pathway_mask = Pathway_Mask\n",
    "# \t\t###gene layer --> pathway layer\n",
    "# \t\tself.sc1 = nn.Linear(In_Nodes, Pathway_Nodes)\n",
    "# \t\t###pathway layer --> hidden layer\n",
    "# \t\tself.sc2 = nn.Linear(Pathway_Nodes, Hidden_Nodes)\n",
    "# \t\t###hidden layer --> hidden layer 2\n",
    "# \t\tself.sc3 = nn.Linear(Hidden_Nodes, Out_Nodes, bias=False)\n",
    "# \t\t###hidden layer 2 + age --> Cox layer\n",
    "# \t\tself.sc4 = nn.Linear(Out_Nodes+1, 1, bias = False)\n",
    "# \t\tself.sc4.weight.data.uniform_(-0.001, 0.001)\n",
    "# \t\t###randomly select a small sub-network\n",
    "# \t\tself.do_m1 = torch.ones(Pathway_Nodes)\n",
    "# \t\tself.do_m2 = torch.ones(Hidden_Nodes)\n",
    "# \t\t###if gpu is being used\n",
    "# \t\tif torch.cuda.is_available():\n",
    "# \t\t\tself.do_m1 = self.do_m1.cuda()\n",
    "# \t\t\tself.do_m2 = self.do_m2.cuda()\n",
    "# \t\t###\n",
    "\n",
    "# \tdef forward(self, x_1, x_2):\n",
    "# \t\t###force the connections between gene layer and pathway layer w.r.t. 'pathway_mask'\n",
    "# \t\tself.sc1.weight.data = self.sc1.weight.data.mul(self.pathway_mask)\n",
    "# \t\tx_1 = self.tanh(self.sc1(x_1))\n",
    "# \t\tif self.training == True: ###construct a small sub-network for training only\n",
    "# \t\t\tx_1 = x_1.mul(self.do_m1)\n",
    "# \t\tx_1 = self.tanh(self.sc2(x_1))\n",
    "# \t\tif self.training == True: ###construct a small sub-network for training only\n",
    "# \t\t\tx_1 = x_1.mul(self.do_m2)\n",
    "# \t\tx_1 = self.tanh(self.sc3(x_1))\n",
    "# \t\t###combine age with hidden layer 2\n",
    "# \t\tx_cat = torch.cat((x_1, x_2), 1)\n",
    "# \t\tlin_pred = self.sc4(x_cat)\n",
    "\t\t\n",
    "# \t\treturn lin_pred\n",
    "\n",
    "\n",
    "\n",
    "# class Cox_PASNet_Model(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, \n",
    "#                  pathway_mask,\n",
    "#                  In_Nodes, \n",
    "#                  Pathway_Nodes, \n",
    "#                  Hidden_Nodes, \n",
    "#                  Out_Nodes, \n",
    "#                  Learning_Rate,\n",
    "#                  L2, \n",
    "#                  Num_Epochs,\n",
    "#                  Dropout_Rate,\n",
    "#                  clin_covs = None,\n",
    "#                  device='cpu', random_state=123):\n",
    "\n",
    "#         self.device = device if torch.cuda.is_available() and device == 'cuda' else 'cpu'\n",
    "#         self.random_state = random_state\n",
    "#         torch.manual_seed(random_state)\n",
    "#         np.random.seed(random_state)\n",
    "\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.model = None\n",
    "#         self.is_fitted_ = False\n",
    "#         self.training_history_ = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "#         self.In_Nodes = In_Nodes\n",
    "#         self.Pathway_Nodes = Pathway_Nodes\n",
    "#         self.Hidden_Nodes = Hidden_Nodes\n",
    "#         self.Out_Nodes = Out_Nodes\n",
    "#         self.Learning_Rate = Learning_Rate\n",
    "#         self.L2 = L2\n",
    "#         self.Num_Epochs = Num_Epochs\n",
    "#         self.Dropout_Rate = Dropout_Rate\n",
    "#         self.clin_covs = clin_covs\n",
    "#         self.pathway_mask = pathway_mask\n",
    "\n",
    "#     def fit(self, X, y): \n",
    "#         # TODO: Include training data stuff\n",
    "#         # Prepare and scale data\n",
    "#         train_x, train_age, train_ytime, train_yevent, \\\n",
    "#         eval_x, eval_age, eval_ytime, eval_yevent = self._prepare_data(X, y, val_split = 0.1)\n",
    "#         pathway_mask = self._prepare_pathway(self.pathway_mask, torch.FloatTensor)\n",
    "\n",
    "#         #train_loader_ = DataLoader(train_dataset_, batch_size=128, shuffle=True)\n",
    "#         #val_loader = DataLoader(val_dataset_, batch_size = 32, shuffle = True)\n",
    "        \n",
    "        \n",
    "#         self.model = Cox_PASNet(self.In_Nodes, self.Pathway_Nodes, self.Hidden_Nodes, self.Out_Nodes, pathway_mask)\n",
    "#         ###if gpu is being used\n",
    "#         if torch.cuda.is_available():\n",
    "#             self.model.cuda()\n",
    "#         ###\n",
    "#         ###optimizer\n",
    "#         opt = optim.Adam(self.model.parameters(), lr=self.Learning_Rate, weight_decay = self.L2)\n",
    "\n",
    "#         #print(self.Num_Epochs)\n",
    "#         #print(train_age)\n",
    "#         #print(train_x)\n",
    "#         #print(train_yevent)\n",
    "#         #print(train_ytime)\n",
    "\n",
    "#         for epoch in range(self.Num_Epochs+1):\n",
    "#             print(epoch)\n",
    "#             self.model.train()\n",
    "#             opt.zero_grad() ###reset gradients to zeros\n",
    "#             ###Randomize dropout masks\n",
    "#             self.model.do_m1 = dropout_mask(self.Pathway_Nodes, self.Dropout_Rate[0])\n",
    "#             self.model.do_m2 = dropout_mask(self.Hidden_Nodes, self.Dropout_Rate[1])\n",
    "            \n",
    "#             pred = self.model(train_x, train_age) ###Forward\n",
    "#             loss = neg_par_log_likelihood(pred, train_ytime, train_yevent) ###calculate loss\n",
    "#             print(loss)\n",
    "#             loss.backward() ###calculate gradients\n",
    "#             opt.step() ###update weights and biases\n",
    "\n",
    "#             self.model.sc1.weight.data = self.model.sc1.weight.data.mul(self.model.pathway_mask) ###force the connections between gene layer and pathway layer\n",
    "\n",
    "#             ###obtain the small sub-network's connections\n",
    "#             do_m1_grad = copy.deepcopy(self.model.sc2.weight._grad.data)\n",
    "#             do_m2_grad = copy.deepcopy(self.model.sc3.weight._grad.data)\n",
    "#             do_m1_grad_mask = torch.where(do_m1_grad == 0, do_m1_grad, torch.ones_like(do_m1_grad))\n",
    "#             do_m2_grad_mask = torch.where(do_m2_grad == 0, do_m2_grad, torch.ones_like(do_m2_grad))\n",
    "#             ###copy the weights\n",
    "#             net_sc2_weight = copy.deepcopy(self.model.sc2.weight.data)\n",
    "#             net_sc3_weight = copy.deepcopy(self.model.sc3.weight.data)\n",
    "\n",
    "#             ###serializing net \n",
    "#             net_state_dict = self.model.state_dict()\n",
    "\n",
    "#             ###Sparse Coding\n",
    "#             ###make a copy for net, and then optimize sparsity level via copied net\n",
    "#             copy_net = copy.deepcopy(self.model)\n",
    "#             copy_state_dict = copy_net.state_dict()\n",
    "#             for name, param in copy_state_dict.items():\n",
    "#                 ###omit the param if it is not a weight matrix\n",
    "#                 if not \"weight\" in name:\n",
    "#                     continue\n",
    "#                 ###omit gene layer\n",
    "#                 if \"sc1\" in name:\n",
    "#                     continue\n",
    "#                 ###stop sparse coding\n",
    "#                 if \"sc4\" in name:\n",
    "#                     break\n",
    "#                 ###sparse coding between the current two consecutive layers is in the trained small sub-network\n",
    "#                 if \"sc2\" in name:\n",
    "#                     active_param = net_sc2_weight.mul(do_m1_grad_mask)\n",
    "#                 if \"sc3\" in name:\n",
    "#                     active_param = net_sc3_weight.mul(do_m2_grad_mask)\n",
    "#                 nonzero_param_1d = active_param[active_param != 0]\n",
    "#                 if nonzero_param_1d.size(0) == 0: ###stop sparse coding between the current two consecutive layers if there are no valid weights\n",
    "#                     break\n",
    "#                 copy_param_1d = copy.deepcopy(nonzero_param_1d)\n",
    "#                 ###set up potential sparsity level in [0, 100)\n",
    "#                 S_set =  torch.arange(100, -1, -1)[1:]\n",
    "#                 copy_param = copy.deepcopy(active_param)\n",
    "#                 S_loss = []\n",
    "#                 for S in S_set:\n",
    "#                     param_mask = s_mask(sparse_level = S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "#                     transformed_param = copy_param.mul(param_mask)\n",
    "#                     copy_state_dict[name].copy_(transformed_param)\n",
    "#                     copy_net.train()\n",
    "#                     y_tmp = copy_net(train_x, train_age)\n",
    "#                     loss_tmp = neg_par_log_likelihood(y_tmp, train_ytime, train_yevent).detach().numpy()\n",
    "#                     S_loss.append(loss_tmp)\n",
    "#                 ###apply cubic interpolation\n",
    "#                 #print(S_set.shape)\n",
    "#                 #print(S_loss.shape)\n",
    "#                 interp_S_loss = interp1d(S_set, S_loss, kind='cubic', axis = 0)\n",
    "#                 interp_S_set = torch.linspace(min(S_set), max(S_set), steps=100)\n",
    "#                 interp_loss = interp_S_loss(interp_S_set)\n",
    "#                 optimal_S = interp_S_set[np.argmin(interp_loss)]\n",
    "#                 optimal_param_mask = s_mask(sparse_level = optimal_S.item(), param_matrix = copy_param, nonzero_param_1D = copy_param_1d, dtype = dtype)\n",
    "#                 if \"sc2\" in name:\n",
    "#                     final_optimal_param_mask = torch.where(do_m1_grad_mask == 0, torch.ones_like(do_m1_grad_mask), optimal_param_mask)\n",
    "#                     optimal_transformed_param = net_sc2_weight.mul(final_optimal_param_mask)\n",
    "#                 if \"sc3\" in name:\n",
    "#                     final_optimal_param_mask = torch.where(do_m2_grad_mask == 0, torch.ones_like(do_m2_grad_mask), optimal_param_mask)\n",
    "#                     optimal_transformed_param = net_sc3_weight.mul(final_optimal_param_mask)\n",
    "#                 ###update weights in copied net\n",
    "#                 copy_state_dict[name].copy_(optimal_transformed_param)\n",
    "#                 ###update weights in net\n",
    "#                 net_state_dict[name].copy_(optimal_transformed_param)\n",
    "\n",
    "#             #if epoch % 200 == 0: \n",
    "#             self.model.train()\n",
    "#             train_pred = self.model(train_x, train_age)\n",
    "#             train_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent).view(1,)\n",
    "\n",
    "#             self.model.eval()\n",
    "#             eval_pred = self.model(eval_x, eval_age)\n",
    "#             eval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent).view(1,)\n",
    "\n",
    "#             train_cindex = c_index(train_pred, train_ytime, train_yevent)\n",
    "#             eval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)\n",
    "#             print(\"Loss in Train: \", train_loss)\n",
    "#             print(\"Loss in Eval: \", eval_loss)\n",
    "            \n",
    "#             print(\"CI in Train: \", train_cindex)\n",
    "#             print(\"CI in Eval: \", eval_cindex)\n",
    "\n",
    "#             #return (train_loss, eval_loss, train_cindex, eval_cindex)\n",
    "#             self.is_fitted_ = True\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         check_is_fitted(self, 'is_fitted_')\n",
    "#         pdata = X.loc[:, self.clin_covs]\n",
    "#         pdata =  torch.FloatTensor(pdata).to(self.device)\n",
    "#         X.drop(self.clin_vars, axis = 1, inplace = True)\n",
    "#         X = torch.FloatTensor(X).to(self.device)\n",
    "#         self.model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             risk_scores = self.model(X, pdata).cpu().numpy()\n",
    "#         return risk_scores.flatten()\n",
    "\n",
    "#     def score(self, X, y):\n",
    "#         check_is_fitted(self, 'is_fitted_')\n",
    "#         preds = self.predict(X)\n",
    "#         y_time = y['time']\n",
    "#         event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "#         y_event = y[event_field] \n",
    "#         return c_index(preds, y_time, y_event)\n",
    "\n",
    "#     # def get_params(self, deep=True):\n",
    "#     #     return {\n",
    "#     #         \"n_features\": self.n_features,\n",
    "#     #         \"hidden_layers\": self.hidden_layers,\n",
    "#     #         \"dropout\": self.dropout,\n",
    "#     #         \"learning_rate\": self.learning_rate,\n",
    "#     #         \"device\": self.device,\n",
    "#     #         \"random_state\": self.random_state,\n",
    "#     #     }\n",
    "\n",
    "#     def set_params(self, **parameters):\n",
    "#         for parameter, value in parameters.items():\n",
    "#             setattr(self, parameter, value)\n",
    "#         return self\n",
    "    \n",
    "#     def clone(self): \n",
    "#         super(self).clone()\n",
    "\n",
    "#     def _prepare_data(self, X, y, val_split = 0.1):\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=42)\n",
    "\n",
    "#         event_field_train = 'status' if 'status' in y_train.dtype.names else 'event'\n",
    "#         #event_field_train = 'BCR_STATUS'\n",
    "#         y_train = pd.DataFrame(y_train).set_index(X_train.index)\n",
    "#         #print(y_train)\n",
    "\n",
    "#         data_train = pd.concat([X_train, y_train], axis = 1, ignore_index=False) \n",
    "#         #print(data_train.loc[:, 'time'])\n",
    "#         X_train, times_train, events_train, pdata_train = self._sort_data(data_train, event_field_train, 'time', self.clin_covs)\n",
    "        \n",
    "#         X_tensor_train = torch.FloatTensor(X_train).to(self.device)\n",
    "#         time_tensor_train = torch.FloatTensor(times_train).to(self.device)\n",
    "#         event_tensor_train = np.ascontiguousarray(events_train).astype(np.float32).to_device(self.device)\n",
    "#         event_tensor_train = torch.from_numpy(event_tensor_train).to(self.device)\n",
    "#         pdata_tensor_train = torch.FloatTensor(pdata_train).to(self.device)\n",
    "        \n",
    "#         event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "#         #event_field_val = 'BCR_STATUS'\n",
    "#         y_val = pd.DataFrame(y_val).set_index(X_val.index)\n",
    "#         data_val = pd.concat([X_val, y_val], axis = 1, ignore_index=False)\n",
    "#         X_val, times_val, events_val, pdata_val = self._sort_data(data_val, event_field_val, 'time', self.clin_covs)\n",
    "        \n",
    "#         X_tensor_val = torch.FloatTensor(X_val).to(self.device)\n",
    "#         time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "#         event_tensor_val= np.ascontiguousarray(events_val).astype(np.float32).to_device(self.device)\n",
    "#         event_tensor_val = torch.FloatTensor(event_tensor_val).to(self.device)\n",
    "#         pdata_tensor_val = torch.FloatTensor(pdata_val).to(self.device)\n",
    "\n",
    "#         # X_scaled_val = self.scaler.transform(X_val)\n",
    "#         # times_val = np.ascontiguousarray(y_val['time']).astype(np.float32)\n",
    "#         # event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "#         # events_val = np.ascontiguousarray(y_val[event_field_val]).astype(np.float32)\n",
    "#         # X_tensor_val = torch.FloatTensor(X_scaled_val).to(self.device)\n",
    "#         # time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "#         # event_tensor_val = torch.FloatTensor(events_val).to(self.device)\n",
    "\n",
    "        \n",
    "#         return X_tensor_train, pdata_tensor_train, time_tensor_train, event_tensor_train, X_tensor_val, pdata_tensor_val, time_tensor_val, event_tensor_val\n",
    "\n",
    "\n",
    "#     def _sort_data(self, data, event_field, times_field, clin_vars = None):\n",
    "#         ''' sort the genomic and clinical data w.r.t. survival time (OS_MONTHS) in descending order\n",
    "#         Input:\n",
    "#             path: path to input dataset (which is expected to be a csv file).\n",
    "#         Output:\n",
    "#             x: sorted genomic inputs.\n",
    "#             ytime: sorted survival time (OS_MONTHS) corresponding to 'x'.\n",
    "#             yevent: sorted censoring status (OS_EVENT) corresponding to 'x', where 1 --> deceased; 0 --> censored.\n",
    "#             age: sorted age corresponding to 'x'.\n",
    "#         '''\n",
    "        \n",
    "#         #data = pd.read_csv(path)\n",
    "#         #(data.info())\n",
    "#         data.sort_values(times_field, ascending = False, inplace = True)\n",
    "#         x = data \n",
    "#         # remove clinical vars\n",
    "#         if clin_vars is not None: \n",
    "#             pData = data.loc[:, self.clin_covs].values\n",
    "#             x = data.drop(self.clin_covs, axis = 1)\n",
    "#         x = x.drop([times_field, event_field], axis = 1)\n",
    "#         intersect_cols = np.intersect1d(self.pathway_mask.columns, x.columns)\n",
    "#         x = x.loc[: , intersect_cols].values\n",
    "#         self.In_Nodes = len(intersect_cols)\n",
    "#         ytime = data.loc[:, [times_field]].values\n",
    "#         yevent = data.loc[:, [event_field]].values\n",
    "        \n",
    "#         self.pathway_mask = self.pathway_mask.loc[:, intersect_cols]\n",
    "#         #print(ytime)\n",
    "#         return(x, ytime, yevent, pData)\n",
    "\n",
    "\n",
    "#     # def _load_data(self, path, dtype):\n",
    "#     #     '''Load the sorted data, and then covert it to a Pytorch tensor.\n",
    "#     #     Input:\n",
    "#     #         path: path to input dataset (which is expected to be a csv file).\n",
    "#     #         dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)\n",
    "#     #     Output:\n",
    "#     #         X: a Pytorch tensor of 'x' from sort_data().\n",
    "#     #         YTIME: a Pytorch tensor of 'ytime' from sort_data().\n",
    "#     #         YEVENT: a Pytorch tensor of 'yevent' from sort_data().\n",
    "#     #         AGE: a Pytorch tensor of 'age' from sort_data().\n",
    "#     #     '''\n",
    "#     #     x, ytime, yevent, pdata = self.sort_data(path)\n",
    "\n",
    "#     #     X = torch.from_numpy(x).type(dtype)\n",
    "#     #     YTIME = torch.from_numpy(ytime).type(dtype)\n",
    "#     #     YEVENT = torch.from_numpy(yevent).type(dtype)\n",
    "#     #     PDATA = torch.from_numpy(pdata).type(dtype)\n",
    "#     #     ###if gpu is being used\n",
    "#     #     if torch.cuda.is_available():\n",
    "#     #         X = X.cuda()\n",
    "#     #         YTIME = YTIME.cuda()\n",
    "#     #         YEVENT = YEVENT.cuda()\n",
    "#     #         PDATA = PDATA.cuda()\n",
    "#     #     ###\n",
    "#     #     return(X, YTIME, YEVENT, PDATA)\n",
    "\n",
    "\n",
    "#     def _prepare_pathway(self, pathway_mask, dtype):\n",
    "#         '''Load a bi-adjacency matrix of pathways, and then covert it to a Pytorch tensor.\n",
    "#         Input:\n",
    "#             path: path to input dataset (which is expected to be a csv file).\n",
    "#             dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)\n",
    "#         Output:\n",
    "#             PATHWAY_MASK: a Pytorch tensor of the bi-adjacency matrix of pathways.\n",
    "#         '''\n",
    "#         #pathway_mask = pd.read_csv(path, index_col = 0).as_matrix()\n",
    "#         pathway_mask = self.pathway_mask.values\n",
    "#         PATHWAY_MASK = torch.from_numpy(pathway_mask).type(dtype)\n",
    "#         ###if gpu is being used\n",
    "#         if torch.cuda.is_available():\n",
    "#             PATHWAY_MASK = PATHWAY_MASK.cuda()\n",
    "#         ###\n",
    "#         return(PATHWAY_MASK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = ModellingProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 17:26:45,840 - INFO - Loading data...\n",
      "2025-01-03 17:26:57,071 - INFO - Found clinical data specification\n",
      "2025-01-03 17:26:57,157 - INFO - Loaded data: 1091 samples, 13215 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1091 entries, Atlanta_2014_Long.PT081 to Stockholm_2016_Ross_Adams.STKHLM9246\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   AGE                1091 non-null   float64\n",
      " 1   TISSUE             1091 non-null   object \n",
      " 2   CLIN_T_STAGE       1091 non-null   object \n",
      " 3   PATH_T_STAGE       1091 non-null   object \n",
      " 4   GLEASON_SCORE      1091 non-null   int64  \n",
      " 5   PRE_OPERATIVE_PSA  1091 non-null   float64\n",
      " 6   MONTH_TO_BCR       1091 non-null   float64\n",
      " 7   BCR_STATUS         1091 non-null   int64  \n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 76.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mp.prepare_data(DATA_CONFIG, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Cox_PASNet_Model(pathway_mask, In_Nodes, \n",
    "                        Pathway_Nodes, \n",
    "                        Hidden_Nodes, \n",
    "                        Out_Nodes, Learning_Rate, L2, Num_EPOCHS, Dropout_Rate, ['AGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241m.\u001b[39mfit(mp\u001b[38;5;241m.\u001b[39mX, mp\u001b[38;5;241m.\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.fit(mp.X, mp.y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
