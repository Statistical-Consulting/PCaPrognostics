{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to use this code\n",
    "## !Use Google Colab:\n",
    "https://colab.research.google.com/drive/1vR5auTG_f2rbNG_gUIrRnWhLo_-6aoAV\n",
    "1. Run chunk 1\n",
    "2. Define the correct file path for the training data in Chunk 2 + run chunk 2\n",
    "3. Run chunk 3\n",
    "\n",
    "### To perform neste resampling:\n",
    "Adapt params in chunk 4 and run it\n",
    "This stores the best model as a pth file which can be ignored\n",
    "### To train model given parameters and save model as .pkl:\n",
    "Adapt paramt in chunk 5 as explained and then run it. The .pkl file is requiered for getting predictions on test data.\n",
    "### To get model predictions on test data:\n",
    "In chunk 6 define the path to the .pkl model file, as well as to expression and pData test data sets. Then run the chunk.\n"
   ],
   "metadata": {
    "id": "WQUuYHFiC5GG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GXLWaSrhIK8",
    "outputId": "1c895271-74f3-4d1e-dc4a-aae6966d540d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting lifelines\n",
      "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting formulaic>=0.2.2 (from lifelines)\n",
      "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
      "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.3/349.3 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.7/115.7 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=221557e77fe2106e0f2b06d0bc124f552dfeef70d475b1ad8975af3ff31d0b9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
      "Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.3/13.3 MB\u001B[0m \u001B[31m29.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed scikit-learn-1.5.2\n",
      "Collecting scikit-survival==0.23.1\n",
      "  Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.0/49.0 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ecos (from scikit-survival==0.23.1)\n",
      "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.4.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.26.4)\n",
      "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (0.6.7.post3)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.5.2)\n",
      "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.1) (0.1.7.post5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival==0.23.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival==0.23.1) (1.17.0)\n",
      "Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/3.9 MB\u001B[0m \u001B[31m53.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m220.1/220.1 kB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ecos, scikit-survival\n",
      "Successfully installed ecos-2.0.14 scikit-survival-0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lifelines\n",
    "!pip install scikit-learn==1.5.2\n",
    "!pip install scikit-survival==0.23.1\n",
    "# Requiered imports\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_random_state\n",
    "from sksurv.util import Surv\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 2\n",
    "# Defining the pathways to the data used for model training.\n",
    "# One expression data file and one pData file is needed.\n",
    "# As for standard input, common genes and intersect genes are used. One is commented out.\n",
    "# /content is the folder which serves as the standard upload folder in google colab\n",
    "#EXPRESSION_DATA_PATH = '/content/exprs_intersect.csv'\n",
    "EXPRESSION_DATA_PATH = '/content/common_genes_knn_imputed.csv'\n",
    "CLINICAL_DATA_PATH = '/content/merged_imputed_pData.csv'"
   ],
   "metadata": {
    "id": "712uU67pDGUL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RWdRTqbgw24"
   },
   "outputs": [],
   "source": [
    "### Chunk 3\n",
    "# Basic logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DeepSurvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch based neural network architecture designed for survival prediction.\n",
    "    This network consists of fully connected layers with ReLU activation,\n",
    "    dropout for regularization, and a final layer that outputs a single\n",
    "    hazard prediction value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, hidden_layers=[32, 16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = n_features\n",
    "        self.model = None\n",
    "\n",
    "        # Build hidden layers with ReLU activation and dropout\n",
    "        for size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = size\n",
    "\n",
    "        # Final layer for hazard prediction\n",
    "        layers.append(nn.Linear(prev_size, 1, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class DeepSurvModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Implementation of the DeepSurv model that integrates\n",
    "    with scikit-learn, specifying  configurable architecture,\n",
    "    training procedures, and evaluation metrics.\n",
    "\n",
    "    The model includes:\n",
    "    - Customizable neural network architecture\n",
    "    - Mini-batch training with early stopping\n",
    "    - CPU/GPU support\n",
    "    - Concordance index evaluation\n",
    "    - Compatibility with scikit-learn's cross-validation and pipeline features\n",
    "    - Reproducible training through seed control\n",
    "\n",
    "    The model follows scikit-learn's estimator interface by implementing\n",
    "    fit(), predict(), get_params() and set_params() methods.\n",
    "    \"\"\"\n",
    "    # Main model class for survival prediction\n",
    "    def __init__(self, n_features=None, hidden_layers=[16, 16], dropout=0.5,\n",
    "                 learning_rate=0.01, device='cpu', random_state=123,\n",
    "                 batch_size=128, num_epochs=100, patience=12):\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device if torch.cuda.is_available() and device == 'cuda' else 'cpu'\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "        self.training_history_ = {'train_loss': [], 'val_loss': []}\n",
    "        self.n_features_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train the model\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.init_network(self.n_features_in_)\n",
    "        self.model.to(self.device)\n",
    "        # seed\n",
    "        np.random.seed(self.random_state)\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.random_state)\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset_, val_dataset_ = self._prepare_data(X, y, val_split=0.1)\n",
    "\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(self.random_state)\n",
    "\n",
    "        train_loader_ = DataLoader(train_dataset_, batch_size=self.batch_size, shuffle=True, generator=generator)\n",
    "        val_loader = DataLoader(val_dataset_, batch_size=32, shuffle=True, generator=generator)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        counter = 0.0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss_ = 0.0\n",
    "            n_batches_ = 0\n",
    "            for X_batch, time_batch, event_batch in train_loader_:\n",
    "                loss = self._train_step(X_batch, time_batch, event_batch)\n",
    "                epoch_loss_ += loss\n",
    "                n_batches_ += 1\n",
    "            avg_train_loss = epoch_loss_ / n_batches_\n",
    "            self.training_history_['train_loss'].append(avg_train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, time_batch, event_batch in val_loader:\n",
    "                    val_loss += self._eval_step(X_batch, time_batch, event_batch)\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            self.training_history_['val_loss'].append(val_loss)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter > self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            risk_scores = self.model(X).cpu().numpy()\n",
    "        return risk_scores.flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        preds = self.predict(X)\n",
    "        return self.c_index(-preds, y)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"n_features\": self.n_features,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"device\": self.device,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"patience\": self.patience\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def clone(self):\n",
    "        super(self).clone()\n",
    "\n",
    "    def _prepare_data(self, X, y, val_split=0.1):\n",
    "        # SPlit and prepare data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=self.random_state)\n",
    "\n",
    "        X_scaled_train = X_train\n",
    "        times_train = np.ascontiguousarray(y_train['time']).astype(np.float32)\n",
    "        event_field_train = 'status' if 'status' in y_train.dtype.names else 'event'\n",
    "        events_train = np.ascontiguousarray(y_train[event_field_train]).astype(np.float32)\n",
    "        X_tensor_train = torch.FloatTensor(X_scaled_train).to(self.device)\n",
    "        time_tensor_train = torch.FloatTensor(times_train).to(self.device)\n",
    "        event_tensor_train = torch.FloatTensor(events_train).to(self.device)\n",
    "\n",
    "        X_scaled_val = X_val\n",
    "        times_val = np.ascontiguousarray(y_val['time']).astype(np.float32)\n",
    "        event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "        events_val = np.ascontiguousarray(y_val[event_field_val]).astype(np.float32)\n",
    "        X_tensor_val = torch.FloatTensor(X_scaled_val).to(self.device)\n",
    "        time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "        event_tensor_val = torch.FloatTensor(events_val).to(self.device)\n",
    "\n",
    "        return TensorDataset(X_tensor_train, time_tensor_train, event_tensor_train), \\\n",
    "              TensorDataset(X_tensor_val, time_tensor_val, event_tensor_val)\n",
    "\n",
    "\n",
    "    def _negative_log_likelihood(self, risk_pred, times, events):\n",
    "        # Calculate loss function\n",
    "        _, idx = torch.sort(times, descending=True)\n",
    "        risk_pred = risk_pred[idx]\n",
    "        events = events[idx]\n",
    "        log_risk = risk_pred\n",
    "        risk = torch.exp(log_risk)\n",
    "        cumsum_risk = torch.cumsum(risk, dim=0)\n",
    "        log_cumsum_risk = torch.log(cumsum_risk + 1e-10)\n",
    "        event_loss = events * (log_risk - log_cumsum_risk)\n",
    "        return -torch.mean(event_loss)\n",
    "\n",
    "    def _train_step(self, X, times, events):\n",
    "        # Single training step\n",
    "        self.optimizer.zero_grad()\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _eval_step(self, X, times, events):\n",
    "        # Single evaluation step\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        return loss.item()\n",
    "\n",
    "    def _check_early_stopping(self, counter):\n",
    "        # Check early stopping conditions\n",
    "        if len(self.training_history_['val_loss']) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        if self.training_history_['val_loss'][-1] < self.training_history_['val_loss'][-2]:\n",
    "            counter = 0.0\n",
    "        else:\n",
    "            counter += 1.0\n",
    "        return counter\n",
    "\n",
    "    def c_index(self, risk_pred, y):\n",
    "        # Calculate concordance index\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.detach().cpu().numpy()\n",
    "        event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "        time = y['time']\n",
    "        event = y[event_field]\n",
    "        if not isinstance(risk_pred, np.ndarray):\n",
    "            risk_pred = risk_pred.detach().cpu().numpy()\n",
    "        if np.isnan(risk_pred).all():\n",
    "            return np.nan\n",
    "        return concordance_index(time, risk_pred, event)\n",
    "\n",
    "    def init_network(self, n_features):\n",
    "        # Initialize network and optimizer\n",
    "        self.model = DeepSurvNet(n_features=n_features, hidden_layers=self.hidden_layers, dropout=self.dropout).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "# Helper functions for survival data handling\n",
    "def _get_survival_subset(y, indices):\n",
    "    \"\"\"Extract survival data subset\"\"\"\n",
    "    subset = np.empty(len(indices), dtype=y.dtype)\n",
    "    event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "    subset[event_field] = y[event_field][indices]\n",
    "    subset['time'] = y['time'][indices]\n",
    "    return subset\n",
    "\n",
    "def _aggregate_results(results):\n",
    "    \"\"\"Aggregates nested CV results.\"\"\"\n",
    "    scores = [res['test_score'] for res in results]\n",
    "    if np.isnan(scores).all():\n",
    "        logger.warning(f\"Found only NaN values in CV-results: {scores}\")\n",
    "        mean_score, std_score = np.nan, np.nan\n",
    "    else:\n",
    "        mean_score = np.nanmean(scores)\n",
    "        std_score = np.nanstd(scores)\n",
    "\n",
    "    logger.info(f\"Aggregated results:\")\n",
    "    logger.info(f\"Mean score: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "    logger.info(f\"Individual scores: {scores}\")\n",
    "\n",
    "    return {\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score,\n",
    "        'fold_results': results\n",
    "    }\n",
    "\n",
    "def nested_resampling(estimator, X, y, groups, param_grid, monitor = None, ss = GridSearchCV,\n",
    "                     outer_cv = LeaveOneGroupOut(), inner_cv = LeaveOneGroupOut(), scoring = None):\n",
    "    \"\"\"Implementation of the nested resampling logic for hyperparameter optimization\"\"\"\n",
    "    # Main nested CV implementation\n",
    "    logger.info(\"Starting nested resampling...\")\n",
    "    logger.info(f\"Data shape: X={X.shape}, groups={len(np.unique(groups))} unique\")\n",
    "\n",
    "    outer_results = []\n",
    "    splits = list(outer_cv.split(X, y, groups))\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        logger.info(f\"\\nOuter fold {i+1}\")\n",
    "\n",
    "        fold_seed = 42 + i\n",
    "        np.random.seed(fold_seed)\n",
    "        torch.manual_seed(fold_seed)\n",
    "\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = _get_survival_subset(y, train_idx)\n",
    "        y_test = _get_survival_subset(y, test_idx)\n",
    "        train_groups = groups[train_idx] if groups is not None else None\n",
    "\n",
    "        test_cohort = groups[test_idx][0] if groups is not None else None\n",
    "        logger.info(f\"Test cohort: {test_cohort}\")\n",
    "\n",
    "        inner_gcv = ss(estimator, param_grid, cv = inner_cv, refit = True, n_jobs=1, verbose = 2)\n",
    "        if monitor is not None:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups, model__monitor = monitor)\n",
    "        else:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups)\n",
    "\n",
    "        inner_cv_results = inner_results.cv_results_\n",
    "        inner_best_params = inner_results.best_params_\n",
    "\n",
    "        outer_model = inner_results.best_estimator_.named_steps['model']\n",
    "        test_score = outer_model.score(X_test, y_test)\n",
    "\n",
    "        logger.info(f\"Best parameters: {inner_best_params}\")\n",
    "        logger.info(f\"Test score: {test_score:.3f}\")\n",
    "\n",
    "        outer_results.append({\n",
    "            'test_cohort': test_cohort,\n",
    "            'test_score': test_score,\n",
    "            'best_params': inner_best_params,\n",
    "            'inner_cv_results': inner_cv_results\n",
    "        })\n",
    "\n",
    "    return _aggregate_results(outer_results)\n",
    "\n",
    "class ModellingProcess():\n",
    "    \"\"\"\n",
    "    This class manages the entire modeling process including data preparation,\n",
    "    nested cross-validation, model training, and result saving. It is a\n",
    "    standardized way of modeling used for several of the implemented mode types\n",
    "    and supports both simple training and complex nested resampling approaches.\n",
    "    Results can be automatically saved and evaluated.\n",
    "    \"\"\"\n",
    "    # Main class for model training pipeline\n",
    "    def __init__(self) -> None:\n",
    "        self.outer_cv = LeaveOneGroupOut()\n",
    "        self.inner_cv = LeaveOneGroupOut()\n",
    "        self.ss = GridSearchCV\n",
    "        self.pipe = None\n",
    "        self.cmplt_model = None\n",
    "        self.cmplt_pipeline = None\n",
    "        self.nrs = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.groups = None\n",
    "        self.path = None\n",
    "        self.fname_cv = None\n",
    "\n",
    "    def prepare_survival_data(self, pdata):\n",
    "        # Convert input data to survival format\n",
    "        status = pdata['BCR_STATUS'].astype(bool).values\n",
    "        time = pdata['MONTH_TO_BCR'].astype(float).values\n",
    "        y = Surv.from_arrays(\n",
    "            event=status,\n",
    "            time=time,\n",
    "            name_event='status',\n",
    "            name_time='time'\n",
    "        )\n",
    "        return y\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Load and preprocess data\n",
    "        exprs = pd.read_csv(EXPRESSION_DATA_PATH, index_col=0)\n",
    "        pdata = pd.read_csv(CLINICAL_DATA_PATH, index_col=0)\n",
    "\n",
    "        clinical_features = ['AGE', 'TISSUE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "        numeric_features = ['AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "        categorical_features = ['TISSUE']\n",
    "\n",
    "        clinical_preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "\n",
    "        clinical_data = clinical_preprocessor.fit_transform(pdata[clinical_features])\n",
    "        clinical_df = pd.DataFrame(\n",
    "            clinical_data,\n",
    "            index=pdata.index\n",
    "        )\n",
    "\n",
    "        exprs.columns = exprs.columns.astype(str)\n",
    "        clinical_df.columns = [f'clinical_{i}' for i in range(clinical_df.shape[1])]\n",
    "\n",
    "        self.X = pd.concat([clinical_df, exprs], axis=1)\n",
    "\n",
    "        target_cols = ['MONTH_TO_BCR', 'BCR_STATUS']\n",
    "        target_df = pdata[target_cols]\n",
    "        self.y = self.prepare_survival_data(target_df)\n",
    "\n",
    "        self.groups = np.array([idx.split('.')[0] for idx in self.X.index])\n",
    "\n",
    "    def do_modelling(self, pipeline_steps, config):\n",
    "        # Main modeling workflow\n",
    "        self._set_seed()\n",
    "\n",
    "        if config.get(\"params_mp\", None) is not None:\n",
    "            self.set_params(config['params_mp'])\n",
    "\n",
    "        if config.get(\"path\", None) is None or config.get(\"fname_cv\", None) is None:\n",
    "            logger.warning(\"Didn't get sufficient path info for saving cv-results\")\n",
    "        else:\n",
    "            self.path = config['path']\n",
    "            self.fname_cv = config['fname_cv']\n",
    "\n",
    "        err, mes = self._check_modelling_prerequs(pipeline_steps)\n",
    "        if err:\n",
    "            logger.error(\"Requirements setup error: %s\", mes)\n",
    "            raise Exception(mes)\n",
    "        else:\n",
    "            self.pipe = Pipeline(pipeline_steps)\n",
    "\n",
    "        param_grid, monitor, do_nested_resampling, refit_hp_tuning = self._get_config_vals(config)\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Start model training...\")\n",
    "            logger.info(f\"Input data shape: X={self.X.shape}\")\n",
    "\n",
    "            if do_nested_resampling:\n",
    "                logger.info(\"Nested resampling...\")\n",
    "                self.nrs = nested_resampling(self.pipe, self.X, self.y, self.groups, param_grid, monitor, self.ss, self.outer_cv, self.inner_cv)\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model = None, cv_results = self.nrs, pipe = None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during nested resampling: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        if refit_hp_tuning:\n",
    "            try:\n",
    "                logger.info(\"Do HP Tuning for complete model; refit + set complete model\")\n",
    "                self.cmplt_model = self.fit_cmplt_model(param_grid)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during complete model training: {str(e)}\")\n",
    "                raise\n",
    "        elif refit_hp_tuning is False and do_nested_resampling is False:\n",
    "            logger.info(\"Fit complete model wo. HP tuning (on default params)\")\n",
    "            self.cmplt_model = self.pipe.fit(self.X, self.y)\n",
    "\n",
    "        return self.nrs, self.cmplt_model, self.cmplt_pipeline\n",
    "\n",
    "    def fit_cmplt_model(self, param_grid, monitor=None):\n",
    "        # Fit final model with best parameters\n",
    "        logger.info(\"Do HP Tuning for complete model\")\n",
    "        res = self.ss(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=self.outer_cv,\n",
    "            n_jobs=1,\n",
    "            verbose=2,\n",
    "            refit=True\n",
    "        )\n",
    "        if monitor is not None:\n",
    "            res.fit(self.X, self.y, groups=self.groups, model__monitor=monitor)\n",
    "        else:\n",
    "            res.fit(self.X, self.y, groups=self.groups)\n",
    "        self.resampling_cmplt = res\n",
    "        self.cmplt_pipeline = res.best_estimator_\n",
    "        self.cmplt_model = res.best_estimator_.named_steps['model']\n",
    "        return self.cmplt_model, res\n",
    "\n",
    "\n",
    "    def save_results(self, path, fname, model=None, cv_results=None, pipe=None):\n",
    "        # Save model and results\n",
    "        if model is None:\n",
    "            logger.warning(\"Won't save any model, since it's not provided\")\n",
    "        else:\n",
    "            # Save the model\n",
    "            model_dir = os.path.join(path, 'model')\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.model.to(torch.device('cpu'))\n",
    "            torch.save(model.model, os.path.join(model_dir, f\"{fname}.pth\"))\n",
    "            logger.info(f\"Saved model to {model_dir}\")\n",
    "\n",
    "        if cv_results is None:\n",
    "            logger.warning(\"Won't save any CV results, since it's not provided\")\n",
    "        else:\n",
    "            # Save cross-validation results\n",
    "            results_dir = os.path.join(path, 'results')\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            results_file = os.path.join(results_dir, f\"{fname}_cv.csv\")\n",
    "            pd.DataFrame(cv_results).to_csv(results_file)\n",
    "            logger.info(f\"Saved CV results to {results_file}\")\n",
    "\n",
    "    def _check_modelling_prerequs(self, pipeline_steps):\n",
    "        # Validate modeling prerequisites\n",
    "        err = False\n",
    "        mes = \"\"\n",
    "        if self.X is None or self.y is None:\n",
    "            mes = mes + \"1) Please call prepare_data() with your preferred config or set X, y, and groups\"\n",
    "            err = True\n",
    "        if not any('model' in tup for tup in pipeline_steps):\n",
    "            mes = mes + \"2) Caution! Your pipeline must include a step named 'model' for the model\"\n",
    "            err = True\n",
    "        return err, mes\n",
    "\n",
    "    def _get_config_vals(self, config):\n",
    "        # Extract configuration values\n",
    "        if config.get(\"params_cv\", None) is None:\n",
    "            logger.warning(\"No param grid for (nested) resampling detected - will fit model with default HPs on complete data\")\n",
    "            return None, False, False, False\n",
    "        if config.get('monitor', None) is None:\n",
    "            logger.info(\"No additional monitoring detected\")\n",
    "        return config['params_cv'], config.get('monitor', None), config.get('do_nested_resampling', True), config.get('refit', True)\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # Set model parameters\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def _set_seed(self, seed = 1234):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        global random_state\n",
    "        random_state = check_random_state(seed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPWJFmaLhPAZ",
    "outputId": "7ed69170-d2b7-4792-edb2-b882a334027e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   6.1s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 22\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 17\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 15\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 19\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 17\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.6s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.6s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.8s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 19\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 16\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any model, since it's not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 16\n",
      "Fitting 9 folds for each of 1 candidates, totalling 9 fits\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 18\n",
      "{'mean_score': 0.6859660771841684, 'std_score': 0.04266504708913971, 'fold_results': [{'test_cohort': 'Atlanta_2014_Long', 'test_score': 0.6442196531791907, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.68099084]), 'std_fit_time': array([1.66598289]), 'mean_score_time': array([0.01086333]), 'std_score_time': array([0.00918202]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.56836488]), 'split1_test_score': array([0.67671518]), 'split2_test_score': array([0.65383381]), 'split3_test_score': array([0.72259811]), 'split4_test_score': array([0.67128766]), 'split5_test_score': array([0.76788686]), 'split6_test_score': array([0.64984472]), 'split7_test_score': array([0.67094161]), 'mean_test_score': array([0.6726841]), 'std_test_score': array([0.05400889]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Belfast_2018_Jain', 'test_score': 0.6164921465968587, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.86062118]), 'std_fit_time': array([0.11859099]), 'mean_score_time': array([0.00687793]), 'std_score_time': array([0.0013627]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.59190751]), 'split1_test_score': array([0.68191268]), 'split2_test_score': array([0.74911774]), 'split3_test_score': array([0.6962111]), 'split4_test_score': array([0.64169555]), 'split5_test_score': array([0.68885191]), 'split6_test_score': array([0.67934783]), 'split7_test_score': array([0.66351671]), 'mean_test_score': array([0.67407013]), 'std_test_score': array([0.04233853]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPC_GENE_2017_Fraser', 'test_score': 0.7297297297297297, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.03724203]), 'std_fit_time': array([0.12047881]), 'mean_score_time': array([0.00740677]), 'std_score_time': array([0.00266999]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.66849711]), 'split1_test_score': array([0.62887636]), 'split2_test_score': array([0.67982034]), 'split3_test_score': array([0.68200271]), 'split4_test_score': array([0.63663023]), 'split5_test_score': array([0.69384359]), 'split6_test_score': array([0.67624224]), 'split7_test_score': array([0.65845427]), 'mean_test_score': array([0.66554586]), 'std_test_score': array([0.02130003]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPGEA_2020_Li', 'test_score': 0.6628168110362528, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.00065035]), 'std_fit_time': array([0.1668848]), 'mean_score_time': array([0.00910348]), 'std_score_time': array([0.00601451]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.5517341]), 'split1_test_score': array([0.60994764]), 'split2_test_score': array([0.63409563]), 'split3_test_score': array([0.68876861]), 'split4_test_score': array([0.5966409]), 'split5_test_score': array([0.781198]), 'split6_test_score': array([0.59083851]), 'split7_test_score': array([0.72257847]), 'mean_test_score': array([0.64697523]), 'std_test_score': array([0.07225905]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CamCap_2016_Ross_Adams', 'test_score': 0.7171853856562923, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.02843565]), 'std_fit_time': array([0.21549829]), 'mean_score_time': array([0.0087975]), 'std_score_time': array([0.00560435]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.67774566]), 'split1_test_score': array([0.59373741]), 'split2_test_score': array([0.6995842]), 'split3_test_score': array([0.64228425]), 'split4_test_score': array([0.63529725]), 'split5_test_score': array([0.74292845]), 'split6_test_score': array([0.59821429]), 'split7_test_score': array([0.63887951]), 'mean_test_score': array([0.65358388]), 'std_test_score': array([0.04744466]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CancerMap_2017_Luca', 'test_score': 0.646494268195148, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.95136955]), 'std_fit_time': array([0.1399078]), 'mean_score_time': array([0.00790665]), 'std_score_time': array([0.00464225]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.68439306]), 'split1_test_score': array([0.60994764]), 'split2_test_score': array([0.66735967]), 'split3_test_score': array([0.65319217]), 'split4_test_score': array([0.71989175]), 'split5_test_score': array([0.7703827]), 'split6_test_score': array([0.6552795]), 'split7_test_score': array([0.70671617]), 'mean_test_score': array([0.68339533]), 'std_test_score': array([0.04582314]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'DKFZ_2018_Gerhauser', 'test_score': 0.7512479201331115, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.26389596]), 'std_fit_time': array([0.33390182]), 'mean_score_time': array([0.01045108]), 'std_score_time': array([0.00719125]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.66849711]), 'split1_test_score': array([0.61407571]), 'split2_test_score': array([0.7047817]), 'split3_test_score': array([0.67564966]), 'split4_test_score': array([0.72462788]), 'split5_test_score': array([0.65102639]), 'split6_test_score': array([0.66071429]), 'split7_test_score': array([0.6483294]), 'mean_test_score': array([0.66846277]), 'std_test_score': array([0.03211816]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'MSKCC_2010_Taylor', 'test_score': 0.7041925465838509, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.93735322]), 'std_fit_time': array([0.20477208]), 'mean_score_time': array([0.00894478]), 'std_score_time': array([0.00569393]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.65982659]), 'split1_test_score': array([0.58095046]), 'split2_test_score': array([0.64449064]), 'split3_test_score': array([0.67115816]), 'split4_test_score': array([0.72801083]), 'split5_test_score': array([0.63956278]), 'split6_test_score': array([0.75374376]), 'split7_test_score': array([0.66891664]), 'mean_test_score': array([0.66833248]), 'std_test_score': array([0.04996125]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Stockholm_2016_Ross_Adams', 'test_score': 0.7013162335470806, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.0178028]), 'std_fit_time': array([0.12346391]), 'mean_score_time': array([0.00881082]), 'std_score_time': array([0.00502791]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.58410405]), 'split1_test_score': array([0.59695932]), 'split2_test_score': array([0.74532225]), 'split3_test_score': array([0.68014116]), 'split4_test_score': array([0.61366712]), 'split5_test_score': array([0.62836577]), 'split6_test_score': array([0.65224626]), 'split7_test_score': array([0.71001553]), 'mean_test_score': array([0.65135268]), 'std_test_score': array([0.05306929]), 'rank_test_score': array([1], dtype=int32)}}]}\n"
     ]
    }
   ],
   "source": [
    "### Chunk 4\n",
    "# Define hyperparameter grid for the nested resampling. To do so, params_cv can be adapted.\n",
    "# refit and do_nested_resampling should be true\n",
    "# fname_cv is the name by which the the results are stored in a csv file ->adapt\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv'  : {\n",
    "        # Define the number and size of hidden layers\n",
    "        'model__hidden_layers': [[512, 256, 128, 64],[512, 256, 128],[512, 128],[512, 256], [256, 128], [1024], [512], [256], [128]],\n",
    "        # Learning rate for optimization\n",
    "        'model__learning_rate': [0.00001, 0.0001],\n",
    "        # Batch size for training\n",
    "        'model__batch_size': [64],\n",
    "        # Number of training epochs\n",
    "        'model__num_epochs': [500],\n",
    "        # Dropout rate for regularization\n",
    "        'model__dropout': [0.2, 0.4],\n",
    "        'model__device': ['cuda']\n",
    "    },\n",
    "    'refit': True,\n",
    "    'do_nested_resampling': True,\n",
    "    'path' : \"\",\n",
    "    'fname_cv' : 'result_intersection'\n",
    "}\n",
    "\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "\n",
    "ds_pipeline_steps = [\n",
    "    ('model', DeepSurvModel())\n",
    "]\n",
    "\n",
    "nstd_res_result, cmplt_model, cmplt_pipeline = mp.do_modelling(ds_pipeline_steps, MODEL_CONFIG)\n",
    "\n",
    "print(nstd_res_result)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 5\n",
    "# Example parameters for the model\n",
    "model_params = {\n",
    "    'hidden_layers': [256, 128],\n",
    "    'learning_rate': 0.00001,\n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 500,\n",
    "    'dropout': 0.2,\n",
    "    'device': 'cuda',\n",
    "    'random_state': 123\n",
    "}\n",
    "\n",
    "# Initialize modeling process and prepare data\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "\n",
    "# Create a new model with the specified parameters\n",
    "model_to_save = DeepSurvModel(**model_params)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model_to_save.fit(mp.X.values, mp.y)\n",
    "\n",
    "# Generate predictions on the training data\n",
    "preds_train = model_to_save.predict(mp.X.values)\n",
    "\n",
    "# Define directory to save the model\n",
    "save_dir = \"/content/my_saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name for the saved model\n",
    "model_file = os.path.join(save_dir, \"deep_surv_model_common_genes_pData.pkl\")\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model_to_save, f)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QM2ffxGXcsMw",
    "outputId": "20cb7154-2771-4574-a22d-75050b4c8454"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 36\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 6\n",
    "# 1. Load the trained model\n",
    "model_file = \"/content/deep_surv_model_common_genes_and_pData[256, 128].pkl\"\n",
    "with open(model_file, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(\"Model successfully loaded.\")\n",
    "\n",
    "# 2. Load test data\n",
    "# Load expression data\n",
    "exprs_test = pd.read_csv('/content/example_exprs.csv', index_col=0)\n",
    "# Load clinical data\n",
    "test_pdata = pd.read_csv('/content/example_pData.csv', index_col=0)\n",
    "\n",
    "# Align indices between expression and clinical data\n",
    "test_pdata.index = exprs_test.index\n",
    "\n",
    "# 3. Prepare clinical features\n",
    "# Numeric features\n",
    "clinical_numeric = test_pdata[['AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']]\n",
    "\n",
    "# Manually create categorical features (leave one variable out)\n",
    "clinical_categorical = pd.DataFrame(index=test_pdata.index)\n",
    "clinical_categorical['TISSUE_Fresh_frozen'] = 1\n",
    "clinical_categorical['TISSUE_Snap_frozen'] = 0\n",
    "\n",
    "# Combine all features (clinical and expression)\n",
    "X_test = pd.concat([clinical_numeric, clinical_categorical, exprs_test], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# 4. Create a survival object for evaluation\n",
    "test_status = test_pdata['BCR_STATUS'].astype(bool).values\n",
    "test_time = test_pdata['MONTH_TO_BCR'].astype(float).values\n",
    "y_test = Surv.from_arrays(\n",
    "    event=test_status,\n",
    "    time=test_time,\n",
    "    name_event='status',\n",
    "    name_time='time'\n",
    ")\n",
    "\n",
    "# 5. Use the test DataFrame for predictions\n",
    "# X_test is already a DataFrame, no conversion to numpy array needed\n",
    "test_predictions = loaded_model.predict(X_test)  # Pass the DataFrame directly\n",
    "\n",
    "\n",
    "# 6. Calculate the C-index on test data\n",
    "test_cindex = loaded_model.c_index(-test_predictions, y_test)\n",
    "\n",
    "\n",
    "# 7. Optional: Save predictions with sample IDs\n",
    "results_df = pd.DataFrame({\n",
    "    'sample_id': X_test.index,\n",
    "    'risk_score': test_predictions\n",
    "})\n",
    "results_df.to_csv('/content/test_predictions_combined.csv')\n",
    "print(test_cindex)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnSYbCwhxn_R",
    "outputId": "9489adab-02f5-4904-b7a2-8b06fb44ac02"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model successfully loaded.\n",
      "0.8722129189653267\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
