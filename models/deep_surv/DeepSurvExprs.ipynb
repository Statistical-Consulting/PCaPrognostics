{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to use this code\n",
    "## !Use Google Colab:\n",
    "https://colab.research.google.com/drive/10wDa-BbR8-PtjY-IDImhhqRnixb8UJU6\n",
    "1. Run chunk 1\n",
    "2. Define the correct file path for the training data in Chunk 2 + run chunk 2\n",
    "3. Run chunk 3\n",
    "\n",
    "### To perform neste resampling:\n",
    "Adapt params in chunk 4 and run it\n",
    "This stores the best model as a pth file which can be ignored\n",
    "### To train model given parameters and save model as .pkl:\n",
    "Adapt paramt in chunk 5 as explained and then run it. The .pkl file is requiered for getting predictions on test data.\n",
    "### To get model predictions on test data:\n",
    "In chunk 6 define the path to the .pkl model file, as well as to expression and pData test data sets. Then run the chunk.\n",
    "\n",
    "### To get survival data and curves:\n",
    "- perform steps 1-3\n",
    "-  In chunk 7: define the pkl file and training data (same as used for the pkl file)\n",
    "- run the chunk 7\n",
    "- in chunk 8: define the model the model stored through chunk 7, as well as the expression data for which survival should be predicted. Dim have to be the same as for the training"
   ],
   "metadata": {
    "id": "eQ6ctBa75Rj4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ltet7xBEgMZg",
    "outputId": "fe6a9772-a114-4a18-ac94-78be79497b27"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting lifelines\n",
      "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting formulaic>=0.2.2 (from lifelines)\n",
      "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
      "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.3/349.3 kB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.7/115.7 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=fd97718f521d0cd6fb9e06edc566d85e9a46f0ffefbb93a5076371cf6f5043ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
      "Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.3/13.3 MB\u001B[0m \u001B[31m26.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed scikit-learn-1.5.2\n",
      "Collecting scikit-survival==0.23.1\n",
      "  Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.0/49.0 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ecos (from scikit-survival==0.23.1)\n",
      "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.4.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.26.4)\n",
      "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (0.6.7.post3)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.5.2)\n",
      "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.1) (0.1.7.post5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival==0.23.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival==0.23.1) (1.17.0)\n",
      "Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/3.9 MB\u001B[0m \u001B[31m96.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m220.1/220.1 kB\u001B[0m \u001B[31m22.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ecos, scikit-survival\n",
      "Successfully installed ecos-2.0.14 scikit-survival-0.23.1\n"
     ]
    }
   ],
   "source": [
    "### Chunk 1\n",
    "# Installing and laoding packages\n",
    "!pip install lifelines\n",
    "!pip install scikit-learn==1.5.2\n",
    "!pip install scikit-survival==0.23.1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_random_state\n",
    "from sksurv.util import Surv\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 2\n",
    "# Defining the pathways to the data used for model training.\n",
    "# One expression data file and one pData file is needed.\n",
    "# As for standard input, common genes and intersect genes are used. One is commented out.\n",
    "# /content is the folder which serves as the standard upload folder in google colab\n",
    "#EXPRESSION_DATA_PATH = '/content/exprs_intersect.csv'\n",
    "EXPRESSION_DATA_PATH = '/content/common_genes_knn_imputed.csv'\n",
    "CLINICAL_DATA_PATH = '/content/merged_imputed_pData.csv'"
   ],
   "metadata": {
    "id": "_6khzrqg5Ijk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LmZY4ugI_oa"
   },
   "outputs": [],
   "source": [
    "# Chunk 3\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DeepSurvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch based neural network architecture designed for survival prediction.\n",
    "    This network consists of fully connected layers with ReLU activation,\n",
    "    dropout for regularization, and a final layer that outputs a single\n",
    "    hazard prediction value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, hidden_layers=[32, 16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = n_features\n",
    "        self.model = None\n",
    "\n",
    "        # Build hidden layers\n",
    "        for size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = size\n",
    "\n",
    "        # Output layer (1 node for hazard prediction)\n",
    "        layers.append(nn.Linear(prev_size, 1, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class DeepSurvModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Implementation of the DeepSurv model that integrates\n",
    "    with scikit-learn, specifying  configurable architecture,\n",
    "    training procedures, and evaluation metrics.\n",
    "\n",
    "    The model includes:\n",
    "    - Customizable neural network architecture\n",
    "    - Mini-batch training with early stopping\n",
    "    - CPU/GPU support\n",
    "    - Concordance index evaluation\n",
    "    - Compatibility with scikit-learn's cross-validation and pipeline features\n",
    "    - Reproducible training through seed control\n",
    "\n",
    "    The model follows scikit-learn's estimator interface by implementing\n",
    "    fit(), predict(), get_params() and set_params() methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=None, hidden_layers=[256,128], dropout=0.4,\n",
    "                 learning_rate=0.0001, device='cpu', random_state=123,\n",
    "                 batch_size=64, num_epochs=500, patience=10):\n",
    "        # Initialize hyperparameters and device settings\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device if torch.cuda.is_available() and device == 'cuda' else 'cpu'\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        # Set random seeds for reproducibility\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "        self.training_history_ = {'train_loss': [], 'val_loss': []}\n",
    "        self.n_features_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Validate input data\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        # Set random seeds at the beginning of training\n",
    "        np.random.seed(self.random_state)\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.random_state)\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Initialize model and data loader\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.init_network(self.n_features_in_)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        train_loader, val_loader = self._prepare_data(X, y, val_split=0.1)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        counter = 0.0\n",
    "        # Training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss_ = 0.0\n",
    "            n_batches_ = 0\n",
    "            for X_batch, time_batch, event_batch in train_loader:\n",
    "                loss = self._train_step(X_batch, time_batch, event_batch)\n",
    "                epoch_loss_ += loss\n",
    "                n_batches_ += 1\n",
    "            avg_train_loss = epoch_loss_ / n_batches_\n",
    "            self.training_history_['train_loss'].append(avg_train_loss)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, time_batch, event_batch in val_loader:\n",
    "                    val_loss += self._eval_step(X_batch, time_batch, event_batch)\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            self.training_history_['val_loss'].append(val_loss)\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter > self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Restore best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict risk scores for new data\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            risk_scores = self.model(X).cpu().numpy()\n",
    "        return risk_scores.flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Calculate concordance index\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        preds = self.predict(X)\n",
    "        return self.c_index(-preds, y)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Return model parameters\n",
    "        return {\n",
    "            \"n_features\": self.n_features,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"device\": self.device,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"patience\": self.patience\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        # Set model parameters\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def clone(self):\n",
    "        super(self).clone()\n",
    "\n",
    "    def _prepare_data(self, X, y, val_split = 0.1):\n",
    "      # Split data into training and validation sets\n",
    "      X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=self.random_state)\n",
    "\n",
    "      X_scaled_train = X_train\n",
    "      times_train = np.ascontiguousarray(y_train['time']).astype(np.float32)\n",
    "      event_field_train = 'status' if 'status' in y_train.dtype.names else 'event'\n",
    "      events_train = np.ascontiguousarray(y_train[event_field_train]).astype(np.float32)\n",
    "      X_tensor_train = torch.FloatTensor(X_scaled_train).to(self.device)\n",
    "      time_tensor_train = torch.FloatTensor(times_train).to(self.device)\n",
    "      event_tensor_train = torch.FloatTensor(events_train).to(self.device)\n",
    "\n",
    "      X_scaled_val = X_val\n",
    "      times_val = np.ascontiguousarray(y_val['time']).astype(np.float32)\n",
    "      event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "      events_val = np.ascontiguousarray(y_val[event_field_val]).astype(np.float32)\n",
    "      X_tensor_val = torch.FloatTensor(X_scaled_val).to(self.device)\n",
    "      time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "      event_tensor_val = torch.FloatTensor(events_val).to(self.device)\n",
    "\n",
    "      # Create DataLoader with reproducible generator\n",
    "      train_dataset = TensorDataset(X_tensor_train, time_tensor_train, event_tensor_train)\n",
    "      val_dataset = TensorDataset(X_tensor_val, time_tensor_val, event_tensor_val)\n",
    "\n",
    "      generator = torch.Generator()\n",
    "      generator.manual_seed(self.random_state)\n",
    "\n",
    "      train_loader = DataLoader(\n",
    "          train_dataset,\n",
    "          batch_size=self.batch_size,\n",
    "          shuffle=True,\n",
    "          generator=generator\n",
    "      )\n",
    "\n",
    "      val_loader = DataLoader(\n",
    "          val_dataset,\n",
    "          batch_size=self.batch_size,\n",
    "          shuffle=True,\n",
    "          generator=generator\n",
    "      )\n",
    "\n",
    "      return train_loader, val_loader\n",
    "\n",
    "    def _negative_log_likelihood(self, risk_pred, times, events):\n",
    "        # Calculate negative log-likelihood loss\n",
    "        _, idx = torch.sort(times, descending=True)\n",
    "        risk_pred = risk_pred[idx]\n",
    "        events = events[idx]\n",
    "        log_risk = risk_pred\n",
    "        risk = torch.exp(log_risk)\n",
    "        cumsum_risk = torch.cumsum(risk, dim=0)\n",
    "        log_cumsum_risk = torch.log(cumsum_risk + 1e-10)\n",
    "        event_loss = events * (log_risk - log_cumsum_risk)\n",
    "        return -torch.mean(event_loss)\n",
    "\n",
    "    def _train_step(self, X, times, events):\n",
    "        # Perform one training step\n",
    "        self.optimizer.zero_grad()\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _eval_step(self, X, times, events):\n",
    "        # Evaluate model on validation data\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        return loss.item()\n",
    "\n",
    "    def _check_early_stopping(self, counter):\n",
    "        if len(self.training_history_['val_loss']) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        if self.training_history_['val_loss'][-1] < self.training_history_['val_loss'][-2]:\n",
    "            counter = 0.0\n",
    "        else:\n",
    "            counter += 1.0\n",
    "        return counter\n",
    "\n",
    "    def c_index(self, risk_pred, y):\n",
    "        # Calculate concordance index\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.detach().cpu().numpy()\n",
    "        event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "        time = y['time']\n",
    "        event = y[event_field]\n",
    "        if not isinstance(risk_pred, np.ndarray):\n",
    "            risk_pred = risk_pred.detach().cpu().numpy()\n",
    "        if np.isnan(risk_pred).all():\n",
    "            return np.nan\n",
    "        return concordance_index(time, risk_pred, event)\n",
    "\n",
    "    def init_network(self, n_features):\n",
    "        # Initialize the neural network and optimizer\n",
    "        self.model = DeepSurvNet(n_features=n_features, hidden_layers=self.hidden_layers, dropout=self.dropout).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "def _get_survival_subset(y, indices):\n",
    "    \"\"\"Extract survival data subset while preserving structure\"\"\"\n",
    "    subset = np.empty(len(indices), dtype=y.dtype)\n",
    "    event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "    subset[event_field] = y[event_field][indices]\n",
    "    subset['time'] = y['time'][indices]\n",
    "    return subset\n",
    "\n",
    "\n",
    "\n",
    "def _aggregate_results(results):\n",
    "    \"\"\"Aggregates nested CV results.\"\"\"\n",
    "    scores = [res['test_score'] for res in results]\n",
    "    if np.isnan(scores).all():\n",
    "        logger.warning(f\"Found only NaN values in CV-results: {scores}\")\n",
    "        mean_score, std_score = np.nan, np.nan\n",
    "    else:\n",
    "        mean_score = np.nanmean(scores)\n",
    "        std_score = np.nanstd(scores)\n",
    "\n",
    "    logger.info(f\"Aggregated results:\")\n",
    "    logger.info(f\"Mean score: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "    logger.info(f\"Individual scores: {scores}\")\n",
    "\n",
    "    return {\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score,\n",
    "        'fold_results': results\n",
    "    }\n",
    "\n",
    "def nested_resampling(estimator, X, y, groups, param_grid, monitor = None, ss = GridSearchCV,\n",
    "                     outer_cv = LeaveOneGroupOut(), inner_cv = LeaveOneGroupOut(), scoring = None):\n",
    "    \"\"\"Implementation of the nested resampling logic for hyperparameter optimization\"\"\"\n",
    "\n",
    "    logger.info(\"Starting nested resampling...\")\n",
    "    logger.info(f\"Data shape: X={X.shape}, groups={len(np.unique(groups))} unique\")\n",
    "\n",
    "    outer_results = []\n",
    "\n",
    "    # Create reproducible splits\n",
    "    splits = list(outer_cv.split(X, y, groups))\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        logger.info(f\"\\nOuter fold {i+1}\")\n",
    "\n",
    "        # Set seeds for this fold\n",
    "        fold_seed = 42 + i\n",
    "        np.random.seed(fold_seed)\n",
    "        torch.manual_seed(fold_seed)\n",
    "\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = _get_survival_subset(y, train_idx)\n",
    "        y_test = _get_survival_subset(y, test_idx)\n",
    "        train_groups = groups[train_idx] if groups is not None else None\n",
    "\n",
    "        test_cohort = groups[test_idx][0] if groups is not None else None\n",
    "        logger.info(f\"Test cohort: {test_cohort}\")\n",
    "\n",
    "        # Perform inner cross-validation using grid search\n",
    "        inner_gcv = ss(estimator, param_grid, cv = inner_cv, refit = True, n_jobs=1, verbose = 2)\n",
    "        if monitor is not None:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups, model__monitor = monitor)\n",
    "        else:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups)\n",
    "\n",
    "        # Retrieve results from inner CV\n",
    "        inner_cv_results = inner_results.cv_results_\n",
    "        inner_best_params = inner_results.best_params_\n",
    "\n",
    "        # Evaluate the best model on the outer test set\n",
    "        outer_model = inner_results.best_estimator_.named_steps['model']\n",
    "        test_score = outer_model.score(X_test, y_test)\n",
    "\n",
    "        logger.info(f\"Best parameters: {inner_best_params}\")\n",
    "        logger.info(f\"Test score: {test_score:.3f}\")\n",
    "\n",
    "        # Append results for this outer fold\n",
    "        outer_results.append({\n",
    "            'test_cohort': test_cohort,\n",
    "            'test_score': test_score,\n",
    "            'best_params': inner_best_params,\n",
    "            'inner_cv_results': inner_cv_results\n",
    "        })\n",
    "\n",
    "    return _aggregate_results(outer_results)\n",
    "\n",
    "\n",
    "\n",
    "class ModellingProcess():\n",
    "    \"\"\"\n",
    "    This class manages the entire modeling process including data preparation,\n",
    "    nested cross-validation, model training, and result saving. It is a\n",
    "    standardized way of modeling used for several of the implemented mode types\n",
    "    and supports both simple training and complex nested resampling approaches.\n",
    "    Results can be automatically saved and evaluated.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.outer_cv = LeaveOneGroupOut()\n",
    "        self.inner_cv = LeaveOneGroupOut()\n",
    "        self.ss = GridSearchCV\n",
    "        self.pipe = None\n",
    "        self.cmplt_model = None\n",
    "        self.cmplt_pipeline = None\n",
    "        self.nrs = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.groups = None\n",
    "        self.path = None\n",
    "        self.fname_cv = None\n",
    "\n",
    "    def prepare_survival_data(self, pdata):\n",
    "        # Convert survival data into structured format\n",
    "        status = pdata['BCR_STATUS'].astype(bool).values\n",
    "        time = pdata['MONTH_TO_BCR'].astype(float).values\n",
    "        y = Surv.from_arrays(\n",
    "            event=status,\n",
    "            time=time,\n",
    "            name_event='status',\n",
    "            name_time='time'\n",
    "        )\n",
    "        return y\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Load the feature matrix (X) and survival data (y)\n",
    "        # If this is used ouside of google colab the pathways need to be ajusted\n",
    "        self.X = pd.read_csv(EXPRESSION_DATA_PATH, index_col=0)\n",
    "        self.y = self.prepare_survival_data(pd.read_csv(CLINICAL_DATA_PATH, index_col=0))\n",
    "        self.groups = np.array([idx.split('.')[0] for idx in self.X.index])\n",
    "\n",
    "    def do_modelling(self, pipeline_steps, config):\n",
    "        # Set random seed for reproducibility\n",
    "        self._set_seed()\n",
    "\n",
    "        # Set parameters for the modelling process if provided in config\n",
    "        if config.get(\"params_mp\", None) is not None:\n",
    "            self.set_params(config['params_mp'])\n",
    "\n",
    "        # Set file paths for saving results if provided\n",
    "        if config.get(\"path\", None) is None or config.get(\"fname_cv\", None) is None:\n",
    "            logger.warning(\"Didn't get sufficient path info for saving cv-results\")\n",
    "        else:\n",
    "            self.path = config['path']\n",
    "            self.fname_cv = config['fname_cv']\n",
    "\n",
    "        # Check modelling prerequisites\n",
    "        err, mes = self._check_modelling_prerequs(pipeline_steps)\n",
    "        if err:\n",
    "            logger.error(\"Requirements setup error: %s\", mes)\n",
    "            raise Exception(mes)\n",
    "        else:\n",
    "            self.pipe = Pipeline(pipeline_steps)\n",
    "\n",
    "        # Extract configuration values for nested resampling\n",
    "        param_grid, monitor, do_nested_resampling, refit_hp_tuning = self._get_config_vals(config)\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Start model training...\")\n",
    "            logger.info(f\"Input data shape: X={self.X.shape}\")\n",
    "\n",
    "            # Perform nested resampling if enabled\n",
    "            if do_nested_resampling:\n",
    "                logger.info(\"Nested resampling...\")\n",
    "                self.nrs = nested_resampling(\n",
    "                    self.pipe, self.X, self.y, self.groups, param_grid, monitor, self.ss, self.outer_cv, self.inner_cv\n",
    "                )\n",
    "                # Save nested resampling results if paths are provided\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model=None, cv_results=self.nrs, pipe=None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during nested resampling: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Perform hyperparameter tuning if enabled\n",
    "        if refit_hp_tuning:\n",
    "            try:\n",
    "                logger.info(\"Do HP Tuning for complete model; refit + set complete model\")\n",
    "                self.fit_cmplt_model(param_grid)\n",
    "                # Save the complete model if paths are provided\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model=self.cmplt_model, cv_results=None, pipe=self.cmplt_pipeline)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during complete model training: {str(e)}\")\n",
    "                raise\n",
    "        elif not refit_hp_tuning and not do_nested_resampling:\n",
    "            # Fit the entire pipeline without hyperparameter tuning\n",
    "            logger.info(\"Fit complete pipeline without HP tuning (on default params)\")\n",
    "            self.cmplt_pipeline = self.pipe.fit(self.X, self.y)\n",
    "            # Save the pipeline if paths are provided\n",
    "            if (self.fname_cv is not None) and (self.path is not None):\n",
    "                self.save_results(self.path, self.fname_cv, model=None, cv_results=None, pipe=self.cmplt_pipeline)\n",
    "\n",
    "        return self.nrs, self.cmplt_model, self.cmplt_pipeline\n",
    "\n",
    "    def fit_cmplt_model(self, param_grid, monitor=None):\n",
    "        # Perform hyperparameter tuning on the full dataset\n",
    "        logger.info(\"Do HP Tuning for complete model\")\n",
    "        res = self.ss(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=self.outer_cv,\n",
    "            n_jobs=1,\n",
    "            verbose=2,\n",
    "            refit=True\n",
    "        )\n",
    "        if monitor is not None:\n",
    "            res.fit(self.X, self.y, groups=self.groups, model__monitor=monitor)\n",
    "        else:\n",
    "            res.fit(self.X, self.y, groups=self.groups)\n",
    "        self.resampling_cmplt = res\n",
    "        self.cmplt_pipeline = res.best_estimator_\n",
    "        self.cmplt_model = res.best_estimator_.named_steps['model']\n",
    "        return res.best_estimator_.named_steps['model'], res\n",
    "\n",
    "    def save_results(self, path, fname, model=None, cv_results=None, pipe=None):\n",
    "        # Save model and results\n",
    "        if model is None:\n",
    "            logger.warning(\"Won't save any model, since it's not provided\")\n",
    "        else:\n",
    "            # Save the model\n",
    "            model_dir = os.path.join(path, 'model')\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.model.to(torch.device('cpu'))\n",
    "            torch.save(model.model, os.path.join(model_dir, f\"{fname}.pth\"))\n",
    "            logger.info(f\"Saved model to {model_dir}\")\n",
    "\n",
    "        if cv_results is None:\n",
    "            logger.warning(\"Won't save any CV results, since it's not provided\")\n",
    "        else:\n",
    "            # Save cross-validation results\n",
    "            results_dir = os.path.join(path, 'results')\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            results_file = os.path.join(results_dir, f\"{fname}_cv.csv\")\n",
    "            pd.DataFrame(cv_results).to_csv(results_file)\n",
    "            logger.info(f\"Saved CV results to {results_file}\")\n",
    "\n",
    "    def _check_modelling_prerequs(self, pipeline_steps):\n",
    "        # Check if data and pipeline are correctly set up\n",
    "        err = False\n",
    "        mes = \"\"\n",
    "        if self.X is None or self.y is None:\n",
    "            mes += \"1) Please call prepare_data() with your preferred config or set X, y, and groups\"\n",
    "            err = True\n",
    "        if not any('model' in tup for tup in pipeline_steps):\n",
    "            mes += \"2) Caution! Your pipeline must include a step named 'model' for the model\"\n",
    "            err = True\n",
    "        return err, mes\n",
    "\n",
    "    def _get_config_vals(self, config):\n",
    "        # Extract configuration values from the config dictionary\n",
    "        if config.get(\"params_cv\", None) is None:\n",
    "            logger.warning(\"No param grid for (nested) resampling detected - will fit model with default HPs on complete data\")\n",
    "            return None, False, False, False\n",
    "        if config.get('monitor', None) is None:\n",
    "            logger.info(\"No additional monitoring detected\")\n",
    "        return config['params_cv'], config.get('monitor', None), config.get('do_nested_resampling', True), config.get('refit', True)\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # Set attributes based on the provided parameters\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def _set_seed(self, seed=1234):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        global random_state\n",
    "        random_state = check_random_state(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 4\n",
    "# Define hyperparameter grid for the nested resampling. To do so, params_cv can be adapted.\n",
    "# refit and do_nested_resampling should be true\n",
    "# fname_cv is the name by which the the results are stored in a csv file ->adapt\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv'  : {\n",
    "        # Define the number and size of hidden layers\n",
    "        'model__hidden_layers': [[512, 256, 128, 64],[512, 256, 128],[512, 128],[512, 256], [256, 128], [1024], [512], [256], [128]],\n",
    "        # Learning rate for optimization\n",
    "        'model__learning_rate': [0.00001, 0.0001],\n",
    "        # Batch size for training\n",
    "        'model__batch_size': [64],\n",
    "        # Number of training epochs\n",
    "        'model__num_epochs': [500],\n",
    "        # Dropout rate for regularization\n",
    "        'model__dropout': [0.2, 0.4],\n",
    "        'model__device': ['cuda']\n",
    "    },\n",
    "    'refit': True,\n",
    "    'do_nested_resampling': True,\n",
    "    'path' : \"\",\n",
    "    'fname_cv' : 'example_name'\n",
    "}\n",
    "\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "\n",
    "ds_pipeline_steps = [\n",
    "    ('model', DeepSurvModel())\n",
    "]\n",
    "\n",
    "nstd_res_result, cmplt_model, cmplt_pipeline = mp.do_modelling(ds_pipeline_steps, MODEL_CONFIG)\n",
    "\n",
    "print(nstd_res_result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PP-lOK6pbP8r",
    "outputId": "214aa5ca-f6a6-4ffc-ed5d-e3783af72a64"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   6.2s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 12\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 17\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 12\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.8s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 15\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 27\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.0s\n",
      "Early stopping at epoch 21\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.7s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.7s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 17\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 18\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.9s\n",
      "Early stopping at epoch 13\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.9s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 15\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 16\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 27\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.8s\n",
      "Early stopping at epoch 14\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any model, since it's not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 17\n",
      "Fitting 9 folds for each of 1 candidates, totalling 9 fits\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 12\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 15\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[128], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any CV results, since it's not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 15\n",
      "{'mean_score': 0.6387820979811811, 'std_score': 0.08108262343902252, 'fold_results': [{'test_cohort': 'Atlanta_2014_Long', 'test_score': 0.6219653179190752, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.68204659]), 'std_fit_time': array([1.70513945]), 'mean_score_time': array([0.00602192]), 'std_score_time': array([0.00320306]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.60602094]), 'split1_test_score': array([0.62058212]), 'split2_test_score': array([0.65736285]), 'split3_test_score': array([0.71583221]), 'split4_test_score': array([0.66249]), 'split5_test_score': array([0.75041597]), 'split6_test_score': array([0.63548137]), 'split7_test_score': array([0.6722916]), 'mean_test_score': array([0.66505963]), 'std_test_score': array([0.0451669]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Belfast_2018_Jain', 'test_score': 0.583467579540878, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.87082857]), 'std_fit_time': array([0.1547931]), 'mean_score_time': array([0.0053806]), 'std_score_time': array([0.0019403]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.65289017]), 'split1_test_score': array([0.67671518]), 'split2_test_score': array([0.728906]), 'split3_test_score': array([0.69012179]), 'split4_test_score': array([0.66009064]), 'split5_test_score': array([0.73211314]), 'split6_test_score': array([0.63121118]), 'split7_test_score': array([0.68410395]), 'mean_test_score': array([0.68201901]), 'std_test_score': array([0.03300894]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPC_GENE_2017_Fraser', 'test_score': 0.4864864864864865, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.06283361]), 'std_fit_time': array([0.16944886]), 'mean_score_time': array([0.0052706]), 'std_score_time': array([0.0014363]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.64624277]), 'split1_test_score': array([0.63350785]), 'split2_test_score': array([0.66827077]), 'split3_test_score': array([0.68200271]), 'split4_test_score': array([0.64222874]), 'split5_test_score': array([0.74459235]), 'split6_test_score': array([0.62888199]), 'split7_test_score': array([0.64529193]), 'mean_test_score': array([0.66137739]), 'std_test_score': array([0.0355049]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPGEA_2020_Li', 'test_score': 0.6323387872954764, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.16843566]), 'std_fit_time': array([0.26650507]), 'mean_score_time': array([0.00743154]), 'std_score_time': array([0.00465567]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.68699422]), 'split1_test_score': array([0.6003826]), 'split2_test_score': array([0.62162162]), 'split3_test_score': array([0.65764547]), 'split4_test_score': array([0.62356705]), 'split5_test_score': array([0.73960067]), 'split6_test_score': array([0.66032609]), 'split7_test_score': array([0.69760378]), 'mean_test_score': array([0.66096769]), 'std_test_score': array([0.04305759]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CamCap_2016_Ross_Adams', 'test_score': 0.6989174560216509, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.07311878]), 'std_fit_time': array([0.38905919]), 'mean_score_time': array([0.00591227]), 'std_score_time': array([0.00388094]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.59566474]), 'split1_test_score': array([0.59444221]), 'split2_test_score': array([0.68711019]), 'split3_test_score': array([0.62078922]), 'split4_test_score': array([0.65155958]), 'split5_test_score': array([0.77620632]), 'split6_test_score': array([0.57259317]), 'split7_test_score': array([0.62672967]), 'mean_test_score': array([0.64063689]), 'std_test_score': array([0.06126977]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CancerMap_2017_Luca', 'test_score': 0.5907757931218341, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.23381877]), 'std_fit_time': array([0.35631955]), 'mean_score_time': array([0.00748345]), 'std_score_time': array([0.00443124]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.65115607]), 'split1_test_score': array([0.61518325]), 'split2_test_score': array([0.67983368]), 'split3_test_score': array([0.63939686]), 'split4_test_score': array([0.68470907]), 'split5_test_score': array([0.79866889]), 'split6_test_score': array([0.66343168]), 'split7_test_score': array([0.71076612]), 'mean_test_score': array([0.6803932]), 'std_test_score': array([0.05244775]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'DKFZ_2018_Gerhauser', 'test_score': 0.7961730449251248, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.99475393]), 'std_fit_time': array([0.22133658]), 'mean_score_time': array([0.00563374]), 'std_score_time': array([0.00316152]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.65578035]), 'split1_test_score': array([0.61327024]), 'split2_test_score': array([0.64345114]), 'split3_test_score': array([0.65864613]), 'split4_test_score': array([0.69079838]), 'split5_test_score': array([0.64622767]), 'split6_test_score': array([0.63819876]), 'split7_test_score': array([0.63314209]), 'mean_test_score': array([0.64743934]), 'std_test_score': array([0.02106499]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'MSKCC_2010_Taylor', 'test_score': 0.672360248447205, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.26619375]), 'std_fit_time': array([0.6970239]), 'mean_score_time': array([0.01197022]), 'std_score_time': array([0.01117592]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.63410405]), 'split1_test_score': array([0.62263391]), 'split2_test_score': array([0.58835759]), 'split3_test_score': array([0.62688483]), 'split4_test_score': array([0.71312585]), 'split5_test_score': array([0.59264196]), 'split6_test_score': array([0.74043261]), 'split7_test_score': array([0.69524131]), 'mean_test_score': array([0.65167776]), 'std_test_score': array([0.05340039]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Stockholm_2016_Ross_Adams', 'test_score': 0.6665541680728991, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.07869321]), 'std_fit_time': array([0.29949523]), 'mean_score_time': array([0.00735539]), 'std_score_time': array([0.0065872]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([128])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [128], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.60462428]), 'split1_test_score': array([0.60098671]), 'split2_test_score': array([0.73804574]), 'split3_test_score': array([0.64549246]), 'split4_test_score': array([0.57510149]), 'split5_test_score': array([0.64196214]), 'split6_test_score': array([0.70382696]), 'split7_test_score': array([0.64324534]), 'mean_test_score': array([0.64416064]), 'std_test_score': array([0.05071761]), 'rank_test_score': array([1], dtype=int32)}}]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhCCXJy3fz_M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e30df75b-25db-40c1-eec6-5655fce7acea"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 47\n"
     ]
    }
   ],
   "source": [
    "### Chunk 5\n",
    "# Define parameters for training the DeepSurv model\n",
    "model_params = {\n",
    "     'hidden_layers': [256, 128],\n",
    "     'learning_rate': 0.00001,\n",
    "     'batch_size': 64,\n",
    "     'num_epochs': 500,\n",
    "     'dropout': 0.2,\n",
    "     'device': 'cuda',\n",
    "     'random_state': 123\n",
    " }\n",
    "# Initialize the ModellingProcess class and load data\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model_to_save = DeepSurvModel(**model_params)\n",
    "\n",
    "# Make predictions on the training data\n",
    "model_to_save.fit(mp.X.values, mp.y)\n",
    "\n",
    "# save\n",
    "preds_train = model_to_save.predict(mp.X.values)\n",
    "save_dir = \"/content/my_saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_file = os.path.join(save_dir, \"deep_surv_model_common_genes.pkl\")\n",
    "\n",
    "with open(model_file, 'wb') as f:\n",
    "     pickle.dump(model_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 6\n",
    "\n",
    "# Load model\n",
    "model_file = \"/content/deep_surv_model_intersect[256, 128].pkl\"\n",
    "with open(model_file, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load Test Data\n",
    "# Expression Data\n",
    "X_test = pd.read_csv('/content/example_exprs.csv', index_col=0)\n",
    "\n",
    "# Clinical Data containing survival data\n",
    "test_pdata = pd.read_csv('/content/example_pData.csv', index_col=0)\n",
    "\n",
    "# Prepare survival data\n",
    "test_status = test_pdata['BCR_STATUS'].astype(bool).values\n",
    "test_time = test_pdata['MONTH_TO_BCR'].astype(float).values\n",
    "y_test = Surv.from_arrays(\n",
    "    event=test_status,\n",
    "    time=test_time,\n",
    "    name_event='status',\n",
    "    name_time='time'\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = loaded_model.predict(X_test.values)\n",
    "print(\"\\First 5:\")\n",
    "print(test_predictions[:5])\n",
    "\n",
    "# Calculate c-index\n",
    "test_cindex = loaded_model.c_index(-test_predictions, y_test)\n",
    "print(\"\\nC-index on test data:\", test_cindex)\n",
    "\n",
    "# Sore results\n",
    "results_df = pd.DataFrame({\n",
    "    'sample_id': X_test.index,\n",
    "    'risk_score': test_predictions\n",
    "})\n",
    "results_df.to_csv('/content/test_predictions.csv')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVTQ2HuO_NJm",
    "outputId": "eae9cf5f-6d5c-4d7a-f176-53bdc8a168e9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\First 5:\n",
      "[ 0.03913932  0.01209671 -0.0101697   0.29160768  0.16612417]\n",
      "\n",
      "C-index on test data: 0.7681590601915999\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Chunk 7\n",
    "\"\"\"\n",
    "This Code chunk adds the baseline hazard to a trained model. To do so the\n",
    "model, as well as training data are loaded, then the breslow baseline hazard\n",
    "is generated and finally the new model with baseline hazard is stored.\n",
    "\"\"\"\n",
    "# Load the pre-trained DeepSurv model\n",
    "model_file = \"/content/deep_surv_model_common_genes[256, 128].pkl\"\n",
    "with open(model_file, 'rb') as f:\n",
    "    deep_surv_model = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Load training data (features and survival information)\n",
    "X_train = pd.read_csv(\"/content/common_genes_knn_imputed.csv\", index_col=0)\n",
    "pData_train = pd.read_csv(\"/content/merged_imputed_pData.csv\", index_col=0)\n",
    "\n",
    "# Extract survival time and event status\n",
    "times_train = pData_train[\"MONTH_TO_BCR\"].values.astype(float)\n",
    "events_train = pData_train[\"BCR_STATUS\"].astype(bool).values\n",
    "\n",
    "# 3) Compute Breslow estimator for baseline hazard\n",
    "def breslow_baseline_hazard(model, X, times, events):\n",
    "    log_risk = model.predict(X)\n",
    "    risk = np.exp(log_risk)\n",
    "\n",
    "    # Ensure numerical stability for small values\n",
    "    risk = np.clip(risk, 1e-10, None)\n",
    "\n",
    "    # Sort data by survival times\n",
    "    order = np.argsort(times)\n",
    "    sorted_times = times[order]\n",
    "    sorted_events = events[order]\n",
    "    sorted_risk = risk[order]\n",
    "\n",
    "    # Identify unique event times\n",
    "    unique_event_times = np.unique(sorted_times[sorted_events == 1])\n",
    "\n",
    "    bhaz = []\n",
    "    at_risk_sum = np.zeros_like(unique_event_times)\n",
    "    event_count = np.zeros_like(unique_event_times)\n",
    "\n",
    "    # Vectorized computation of baseline hazard\n",
    "    for i, t in enumerate(unique_event_times):\n",
    "        at_risk = sorted_risk[sorted_times >= t]\n",
    "        at_risk_sum[i] = at_risk.sum()\n",
    "        event_count[i] = np.sum((sorted_times == t) & (sorted_events == 1))\n",
    "\n",
    "    bhaz = event_count / np.maximum(at_risk_sum, 1e-8)\n",
    "    cbhaz = np.cumsum(bhaz)\n",
    "\n",
    "    return unique_event_times, bhaz, cbhaz\n",
    "\n",
    "# Compute the baseline hazard using the Breslow method\n",
    "unique_times, bhaz, cbhaz = breslow_baseline_hazard(\n",
    "    model=deep_surv_model,\n",
    "    X=X_train.values,\n",
    "    times=times_train,\n",
    "    events=events_train\n",
    ")\n",
    "\n",
    "\n",
    "# 4) Save the baseline hazard in the model object\n",
    "deep_surv_model.unique_event_times_ = unique_times\n",
    "deep_surv_model.cum_baseline_hazard_ = cbhaz\n",
    "\n",
    "# Optionally save the model with the baseline hazard included\n",
    "with open(\"/content/deep_surv_model_common_genes[256, 128]_inclBH.pkl\", 'wb') as f:\n",
    "    pickle.dump(deep_surv_model, f)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "X6vuvuj1dUjL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Chunk 8\n",
    "\"\"\"This chunk predicts survival based on the prev. stored model\"\"\"\n",
    "# Load the model with stored baseline hazard\n",
    "with open(\"/content/deep_surv_model_common_genes[256, 128]_inclBH.pkl\", 'rb') as f:\n",
    "    deep_surv_model_with_bh = pickle.load(f)\n",
    "\n",
    "# Load new test data\n",
    "X_test = pd.read_csv(\"/content/intersect_genes_test_cohort1_low_risk.csv\", index_col=0)\n",
    "\n",
    "\n",
    "# Compute survival function for test patients\n",
    "def predict_survival(model, X_new):\n",
    "    \"\"\"\n",
    "    Erwartet, dass model.unique_event_times_ und model.cum_baseline_hazard_\n",
    "    vorhanden sind.\n",
    "    \"\"\"\n",
    "    # Compute risk scores\n",
    "    log_risk = model.predict(X_new)\n",
    "    risk = np.exp(log_risk)\n",
    "\n",
    "    # Compute survival probabilities S(t) = exp(-Lambda_0(t) * risk)\n",
    "    times = model.unique_event_times_\n",
    "    cbhaz = model.cum_baseline_hazard_\n",
    "\n",
    "    surv_list = []\n",
    "    for lam_0_t in cbhaz:\n",
    "        surv_list.append(np.exp(-lam_0_t * risk))\n",
    "\n",
    "\n",
    "    surv = np.vstack(surv_list)\n",
    "    return times, surv\n",
    "\n",
    "# Generate survival curves for test patients\n",
    "times_vec, surv_mat = predict_survival(deep_surv_model_with_bh, X_test.values)\n",
    "\n",
    "print(\"Survival-Kurven berechnet!\")\n",
    "print(f\"surv_mat.shape = {surv_mat.shape}, times_vec.shape = {times_vec.shape}\")\n",
    "\n",
    "# Plot survival curves (demo)\n",
    "plt.figure(figsize=(7,5))\n",
    "if X_test.shape[0] == 1:\n",
    "    plt.step(times_vec, surv_mat[:,0], where=\"post\", label=\"Patient\")\n",
    "else:\n",
    "    for i, pat_id in enumerate(X_test.index):\n",
    "        plt.step(times_vec, surv_mat[:, i], where=\"post\", label=str(pat_id))\n",
    "\n",
    "\n",
    "\n",
    "surv_vec = surv_mat[:, 0]\n",
    "df_single = pd.DataFrame({\n",
    "    'time': times_vec,\n",
    "    'survival': surv_vec\n",
    "})\n",
    "df_single.to_csv(\"patient_survival_deep_surv_exprs_example.csv\", index=False)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "58ab_bDIhVUk"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
