{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## !Use Google Colab:\n",
    "https://colab.research.google.com/drive/1rt6yOA0omDJZ1wvGXLTa8UbiEU1QXd3e#scrollTo=4oludV5HDQDH\n",
    "\n",
    "##### Code to fit DeepSurv with pretrained Autoencoder.\n",
    "\n",
    "How to use this code:\n",
    "\n",
    "1. Upload data sets to content pane. To run the code without modifications the names should be:\n",
    "* exprs_intersect.csv for gene data\n",
    "* merged_imputed_pData.csv for clinical data\n",
    "\n",
    "2. In addition, upload the pretrained models to the content pane. The models should be obtained the models-folder from from this notebook: https://colab.research.google.com/drive/1kOvHaFqIrlJQg6Zgy395caf_GaKFTz-W?usp=sharing.\n",
    "\n",
    "3. Adapt model parameters and modelling process parameters in MODEL_CONFIG:\n",
    "* To run the DeepSurv with Autoencoder representations and with or without clinical data, please refer to chunk 5\n",
    "* To perform nested resampling set 'do_nested_resampling' in MODEL_CONFIG to True\n",
    "* To train final model set 'refit' in MODEL_CONFIG to True\n",
    "* Adapt model hyperparameters to your liking\n",
    "\n",
    "4. Run the Notebook\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "gAK8yGQ8xHKC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GXLWaSrhIK8",
    "outputId": "b0a0e335-2c54-44bb-c2cb-b207b9a1b9ab"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: lifelines in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.11/dist-packages (from lifelines) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.1.1)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mRequirement already satisfied: scikit-survival==0.23.1 in /usr/local/lib/python3.11/dist-packages (0.23.1)\n",
      "Requirement already satisfied: ecos in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.0.14)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.4.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.26.4)\n",
      "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (0.6.7.post3)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.5.2)\n",
      "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.1) (0.1.7.post5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival==0.23.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival==0.23.1) (1.17.0)\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting sympy\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0mInstalling collected packages: sympy\n",
      "\u001B[33mWARNING: Ignoring invalid distribution ~ympy (/usr/local/lib/python3.11/dist-packages)\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "torch 2.5.1+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed sympy\n"
     ]
    }
   ],
   "source": [
    "### Chunk 1\n",
    "# Installing and laoding packages\n",
    "!pip install lifelines\n",
    "!pip install scikit-learn==1.5.2\n",
    "!pip install scikit-survival==0.23.1\n",
    "!pip install --upgrade sympy\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_random_state\n",
    "from sksurv.util import Surv\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RWdRTqbgw24"
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DeepSurvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch based neural network architecture designed for survival prediction.\n",
    "    This network consists of fully connected layers with ReLU activation,\n",
    "    dropout for regularization, and a final layer that outputs a single\n",
    "    hazard prediction value.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, hidden_layers=[32, 16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = n_features\n",
    "        self.model = None\n",
    "\n",
    "        # Build hidden layers\n",
    "        for size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = size\n",
    "\n",
    "        # Output layer (1 node for hazard prediction)\n",
    "        layers.append(nn.Linear(prev_size, 1, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class DeepSurvModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Implementation of the DeepSurv model that integrates\n",
    "    with scikit-learn, specifying  configurable architecture,\n",
    "    training procedures, and evaluation metrics.\n",
    "\n",
    "    The model includes:\n",
    "    - Customizable neural network architecture\n",
    "    - Mini-batch training with early stopping\n",
    "    - CPU/GPU support\n",
    "    - Concordance index evaluation\n",
    "    - Compatibility with scikit-learn's cross-validation and pipeline features\n",
    "    - Reproducible training through seed control\n",
    "\n",
    "    The model follows scikit-learn's estimator interface by implementing\n",
    "    fit(), predict(), get_params() and set_params() methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=None, hidden_layers=[16, 16], dropout=0.5,\n",
    "                 learning_rate=0.01, device='cpu', random_state=123,\n",
    "                 batch_size=128, num_epochs=100, patience=15):\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device if torch.cuda.is_available() and device == 'cuda' else 'cpu'\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "        self.training_history_ = {'train_loss': [], 'val_loss': []}\n",
    "        self.n_features_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "      # Input validation for X and y\n",
    "      X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "      self.n_features_in_ = X.shape[1]\n",
    "      print(self.n_features_in_)\n",
    "      self.init_network(self.n_features_in_)\n",
    "      self.model.to(self.device)\n",
    "\n",
    "      train_dataset_, val_dataset_ = self._prepare_data(X, y, val_split = 0.1)\n",
    "      train_loader_ = DataLoader(train_dataset_, batch_size=self.batch_size, shuffle=True)\n",
    "      val_loader = DataLoader(val_dataset_, batch_size = 32, shuffle = True)\n",
    "\n",
    "      best_val_loss = float('inf')\n",
    "      best_model_state = None\n",
    "      counter = 0.0\n",
    "      for epoch in range(self.num_epochs):\n",
    "          self.model.train()\n",
    "          epoch_loss_ = 0.0\n",
    "          n_batches_ = 0\n",
    "          for X_batch, time_batch, event_batch in train_loader_:\n",
    "              loss = self._train_step(X_batch, time_batch, event_batch)\n",
    "              epoch_loss_ += loss\n",
    "              n_batches_ += 1\n",
    "          avg_train_loss = epoch_loss_ / n_batches_\n",
    "          self.training_history_['train_loss'].append(avg_train_loss)\n",
    "\n",
    "          # Validation\n",
    "          self.model.eval()\n",
    "          val_loss = 0.0\n",
    "          with torch.no_grad():\n",
    "              for X_batch, time_batch, event_batch in val_loader:\n",
    "                  val_loss += self._eval_step(X_batch, time_batch, event_batch)\n",
    "\n",
    "          val_loss = val_loss / len(val_loader)\n",
    "          self.training_history_['val_loss'].append(val_loss)\n",
    "\n",
    "          # Save best model\n",
    "          if val_loss < best_val_loss:\n",
    "              best_val_loss = val_loss\n",
    "              best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "              counter = 0\n",
    "          else:\n",
    "              counter += 1\n",
    "\n",
    "          if counter > self.patience:\n",
    "              print(f\"Early stopping at epoch {epoch+1}\")\n",
    "              break\n",
    "\n",
    "      # Restore best model\n",
    "      if best_model_state is not None:\n",
    "          self.model.load_state_dict(best_model_state)\n",
    "\n",
    "      self.is_fitted_ = True\n",
    "      return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict risk scores for new data\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            risk_scores = self.model(X).cpu().numpy()\n",
    "        return risk_scores.flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Calculate concordance index\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        preds = self.predict(X)\n",
    "        return self.c_index(-preds, y)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Return model parameters\n",
    "        return {\n",
    "            \"n_features\": self.n_features,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"device\": self.device,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"patience\": self.patience\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        # Set model parameters\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def clone(self):\n",
    "        super(self).clone()\n",
    "\n",
    "    def _prepare_data(self, X, y, val_split = 0.1):\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=42)\n",
    "        X_scaled_train = X_train\n",
    "        times_train = np.ascontiguousarray(y_train['time']).astype(np.float32)\n",
    "        event_field_train = 'status' if 'status' in y_train.dtype.names else 'event'\n",
    "        events_train = np.ascontiguousarray(y_train[event_field_train]).astype(np.float32)\n",
    "        X_tensor_train = torch.FloatTensor(X_scaled_train).to(self.device)\n",
    "        time_tensor_train = torch.FloatTensor(times_train).to(self.device)\n",
    "        event_tensor_train = torch.FloatTensor(events_train).to(self.device)\n",
    "\n",
    "        X_scaled_val = X_val\n",
    "        times_val = np.ascontiguousarray(y_val['time']).astype(np.float32)\n",
    "        event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "        events_val = np.ascontiguousarray(y_val[event_field_val]).astype(np.float32)\n",
    "        X_tensor_val = torch.FloatTensor(X_scaled_val).to(self.device)\n",
    "        time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "        event_tensor_val = torch.FloatTensor(events_val).to(self.device)\n",
    "\n",
    "        return TensorDataset(X_tensor_train, time_tensor_train, event_tensor_train), TensorDataset(X_tensor_val, time_tensor_val, event_tensor_val)\n",
    "\n",
    "    def _negative_log_likelihood(self, risk_pred, times, events):\n",
    "        # Calculate negative log-likelihood loss\n",
    "        _, idx = torch.sort(times, descending=True)\n",
    "        risk_pred = risk_pred[idx]\n",
    "        events = events[idx]\n",
    "        log_risk = risk_pred\n",
    "        risk = torch.exp(log_risk)\n",
    "        cumsum_risk = torch.cumsum(risk, dim=0)\n",
    "        log_cumsum_risk = torch.log(cumsum_risk + 1e-10)\n",
    "        event_loss = events * (log_risk - log_cumsum_risk)\n",
    "        return -torch.mean(event_loss)\n",
    "\n",
    "    def _train_step(self, X, times, events):\n",
    "        # Perform one training step\n",
    "        self.optimizer.zero_grad()\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _eval_step(self, X, times, events):\n",
    "        # Evaluate model on validation data\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        return loss.item()\n",
    "\n",
    "    def _check_early_stopping(self, counter):\n",
    "        if len(self.training_history_['val_loss']) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        if self.training_history_['val_loss'][-1] < self.training_history_['val_loss'][-2]:\n",
    "            counter = 0.0\n",
    "        else:\n",
    "            counter += 1.0\n",
    "        return counter\n",
    "\n",
    "    def c_index(self, risk_pred, y):\n",
    "        # Calculate concordance index\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.detach().cpu().numpy()\n",
    "        event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "        time = y['time']\n",
    "        event = y[event_field]\n",
    "        if not isinstance(risk_pred, np.ndarray):\n",
    "            risk_pred = risk_pred.detach().cpu().numpy()\n",
    "        if np.isnan(risk_pred).all():\n",
    "            return np.nan\n",
    "        return concordance_index(time, risk_pred, event)\n",
    "\n",
    "    def init_network(self, n_features):\n",
    "        # Initialize the neural network and optimizer\n",
    "        self.model = DeepSurvNet(n_features=n_features, hidden_layers=self.hidden_layers, dropout=self.dropout).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def _get_survival_subset(y, indices):\n",
    "    \"\"\"\n",
    "    Extracts a subset of the survival dataset\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): Structured array containing survival data with fields 'time' and 'status' (or 'event').\n",
    "        indices: Indices of the subset.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the prognostic endpoint (BCR, MONTH_TO_BCR)\n",
    "    \"\"\"\n",
    "    subset = np.empty(len(indices), dtype=y.dtype)\n",
    "    event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "    subset[event_field] = y[event_field][indices]\n",
    "    subset['time'] = y['time'][indices]\n",
    "    return subset\n",
    "\n",
    "def _aggregate_results(results):\n",
    "    \"\"\"\n",
    "    Aggregates nested cross-validation results.\n",
    "\n",
    "    Args:\n",
    "        results (list of dict): A list of dictionaries containing results from each CV fold.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict with aggr. infos\n",
    "    \"\"\"\n",
    "    scores = [res['test_score'] for res in results]\n",
    "    if np.isnan(scores).all():\n",
    "        logger.warning(f\"Found only NaN values in CV-results: {scores}\")\n",
    "        mean_score, std_score = np.nan, np.nan\n",
    "    else:\n",
    "        mean_score = np.nanmean(scores)\n",
    "        std_score = np.nanstd(scores)\n",
    "\n",
    "    logger.info(f\"Aggregated results:\")\n",
    "    logger.info(f\"Mean score: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "    logger.info(f\"Individual scores: {scores}\")\n",
    "\n",
    "    return {\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score,\n",
    "        'fold_results': results\n",
    "    }\n",
    "\n",
    "def nested_resampling(estimator, X, y, groups, param_grid, monitor = None, ss = GridSearchCV, outer_cv = LeaveOneGroupOut(), inner_cv = LeaveOneGroupOut(), scoring = None):\n",
    "    \"\"\"\n",
    "    Performs nested resampling using Leave-One-Group-Out.\n",
    "\n",
    "    Args:\n",
    "        estimator (sklearn estimator/sklearn pipeline): The base model/pipeline\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (np.ndarray): Survival outcome data.\n",
    "        groups (array): Group labels for each sample\n",
    "        param_grid (dict): Hyperparameter grid for the inner cross-validation.\n",
    "        monitor (optional): Monitoring parameter for early stopping (default: None).\n",
    "        ss (class): The search strategy class (default: GridSearchCV).\n",
    "        outer_cv (sklearn CV splitter): Outer cross-validation splitter (default: LeaveOneGroupOut).\n",
    "        inner_cv (sklearn CV splitter): Inner cross-validation splitter (default: LeaveOneGroupOut).\n",
    "        scoring (optional): Scoring function for model evaluation (default: None).\n",
    "\n",
    "    Returns:\n",
    "        dict: Aggregated results from nested cross-validation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting nested resampling...\")\n",
    "    logger.info(f\"Data shape: X={X.shape}, groups={len(np.unique(groups))} unique\")\n",
    "\n",
    "    outer_results = []\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y, groups)):\n",
    "        logger.info(f\"\\nOuter fold {i+1}\")\n",
    "\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = _get_survival_subset(y, train_idx)\n",
    "        y_test = _get_survival_subset(y, test_idx)\n",
    "        train_groups = groups[train_idx] if groups is not None else None\n",
    "\n",
    "        test_cohort = groups[test_idx][0] if groups is not None else None\n",
    "        logger.info(f\"Test cohort: {test_cohort}\")\n",
    "\n",
    "        inner_gcv = ss(estimator, param_grid, cv = inner_cv, refit = True, n_jobs=4, verbose = 2)\n",
    "        if monitor is not None:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups, model__monitor = monitor)\n",
    "            logger.info(\n",
    "                f\"number of iterations early stopping: {inner_results.best_estimator_.named_steps['model'].n_estimators_}\")\n",
    "\n",
    "        else:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups)\n",
    "\n",
    "        inner_cv_results = inner_results.cv_results_\n",
    "        inner_best_params = inner_results.best_params_\n",
    "\n",
    "        outer_model = inner_results.best_estimator_\n",
    "        test_score = outer_model.score(X_test, y_test)\n",
    "\n",
    "        logger.info(f\"Best parameters: {inner_best_params}\")\n",
    "        logger.info(f\"Test score: {test_score:.3f}\")\n",
    "\n",
    "        outer_results.append({\n",
    "            'test_cohort': test_cohort,\n",
    "            'test_score': test_score,\n",
    "            'best_params': inner_best_params,\n",
    "            'inner_cv_results': inner_cv_results\n",
    "        })\n",
    "\n",
    "    return _aggregate_results(outer_results)\n",
    "\n",
    "\n",
    "class ModellingProcess():\n",
    "    \"\"\"\n",
    "    Class to handle the full modelling process for python models with sklearn-interface. Includes data preparation, cross-validation,\n",
    "    hyperparameter tuning, model fitting, and result saving.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.outer_cv = LeaveOneGroupOut()\n",
    "        self.inner_cv = LeaveOneGroupOut()\n",
    "        self.ss = GridSearchCV\n",
    "        self.pipe = None\n",
    "        self.cmplt_model = None\n",
    "        self.cmplt_pipeline = None\n",
    "        self.nrs = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.groups = None\n",
    "        self.path = None\n",
    "        self.fname_cv = None\n",
    "\n",
    "    def prepare_survival_data(self, pdata):\n",
    "        status = pdata['BCR_STATUS'].astype(bool).values\n",
    "        time = pdata['MONTH_TO_BCR'].astype(float).values\n",
    "        y = Surv.from_arrays(\n",
    "            event=status,\n",
    "            time=time,\n",
    "            name_event='status',\n",
    "            name_time='time'\n",
    "        )\n",
    "        return y\n",
    "\n",
    "    def prepare_data(self, config):\n",
    "        X = pd.read_csv('/content/exprs_intersect.csv', index_col=0)\n",
    "        pdata = pd.read_csv('/content/merged_imputed_pData.csv', index_col=0)\n",
    "\n",
    "        self.y = self.prepare_survival_data(pdata)\n",
    "        self.groups = np.array([idx.split('.')[0] for idx in X.index])\n",
    "\n",
    "        if config.get('clinical_covs', None) is not None:\n",
    "                logger.info('Found clinical data specification')\n",
    "                pdata['AGE'] = pd.to_numeric(pdata['AGE'], errors='coerce')\n",
    "                clin_data = pdata.loc[:, config['clinical_covs']]\n",
    "                cat_cols = clin_data.select_dtypes(exclude=['number']).columns\n",
    "                num_cols = clin_data.select_dtypes(exclude=['object']).columns\n",
    "                clin_data_cat = clin_data.loc[:, cat_cols]\n",
    "                if config.get('requires_ohenc', True) is True:\n",
    "                    ohc = OneHotEncoder()\n",
    "                    clin_data_cat = ohc.fit_transform(clin_data_cat)\n",
    "                    clin_data_cat = pd.DataFrame.sparse.from_spmatrix(clin_data_cat, columns=ohc.get_feature_names_out()).set_index(X.index)\n",
    "                clin_data_num = clin_data.loc[:, num_cols]\n",
    "\n",
    "                if config.get('only_pData', False) is not False:\n",
    "                    logger.info('Only uses pData')\n",
    "                    self.X = pd.concat([clin_data_cat, clin_data_num], axis = 1)\n",
    "                else:\n",
    "                    self.X = pd.concat([clin_data_cat, clin_data_num, X], axis = 1)\n",
    "        else:\n",
    "          self.X = X\n",
    "\n",
    "\n",
    "    def do_modelling(self, pipeline_steps, config):\n",
    "        \"\"\"\n",
    "        Executes the complete modeling process, including pipeline creation, nested resampling, and final model fitting.\n",
    "\n",
    "        Args:\n",
    "            pipeline_steps (list): List of (name, transformer) tuples for creating the pipeline --> objects need to adhere to scikit learn interface /API.\n",
    "            config (dict): Configuration for the modeling process, including parameters for cross-validation,\n",
    "                           hyperparameter tuning, and result saving.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Nested resampling results, final model, and complete, final pipeline.\n",
    "        \"\"\"\n",
    "        self._set_seed()\n",
    "\n",
    "        if config.get(\"params_mp\", None) is not None:\n",
    "            self.set_params(config['params_mp'])\n",
    "\n",
    "        if config.get(\"path\", None) is None or config.get(\"fname_cv\", None) is None:\n",
    "            logger.warning(\"Didn't get sufficient path info for saving cv-results\")\n",
    "        else:\n",
    "            self.path = config['path']\n",
    "            self.fname_cv = config['fname_cv']\n",
    "\n",
    "        err, mes = self._check_modelling_prerequs(pipeline_steps)\n",
    "        if err:\n",
    "            logger.error(\"Requirements setup error: %s\", mes)\n",
    "            raise Exception(mes)\n",
    "        else:\n",
    "            self.pipe = Pipeline(pipeline_steps)\n",
    "\n",
    "        param_grid, monitor, do_nested_resampling, refit_hp_tuning = self._get_config_vals(config)\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Start model training...\")\n",
    "            logger.info(f\"Input data shape: X={self.X.shape}\")\n",
    "\n",
    "            if do_nested_resampling:\n",
    "                logger.info(\"Nested resampling...\")\n",
    "                self.nrs = nested_resampling(self.pipe, self.X, self.y, self.groups, param_grid, monitor, self.ss, self.outer_cv, self.inner_cv)\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model = None, cv_results = self.nrs, pipe = None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during nested resampling: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        if refit_hp_tuning:\n",
    "            try:\n",
    "                logger.info(\"Do HP Tuning for complete model; refit + set complete model\")\n",
    "                self.fit_cmplt_model(param_grid)\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model = self.cmplt_model, cv_results = None, pipe = self.cmplt_pipeline)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during complete model training: {str(e)}\")\n",
    "                raise\n",
    "        elif refit_hp_tuning is False and do_nested_resampling is False:\n",
    "            logger.info(\"Fit complete model wo. HP tuning (on default params)\")\n",
    "            self.cmplt_model = self.pipe.fit(self.X, self.y)\n",
    "\n",
    "        return self.nrs, self.cmplt_model, self.cmplt_pipeline\n",
    "\n",
    "\n",
    "    def fit_cmplt_model(self, param_grid, monitor=None):\n",
    "        \"\"\"\n",
    "        Performs hyperparameter tuning and fits the final model on all of group A.\n",
    "\n",
    "        Args:\n",
    "            param_grid (dict): Parameter grid for GridSearchCV.\n",
    "            monitor (optional): Additional monitor object for evaluation during training.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The best model and the complete resampling result.\n",
    "        \"\"\"\n",
    "        logger.info(\"Do HP Tuning for complete model\")\n",
    "        res = self.ss(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=self.outer_cv,\n",
    "            n_jobs=1,  # Changed from -1 to 1\n",
    "            verbose=2,\n",
    "            refit=True\n",
    "        )\n",
    "        if monitor is not None:\n",
    "            res.fit(self.X, self.y, groups=self.groups, model__monitor=monitor)\n",
    "        else:\n",
    "            res.fit(self.X, self.y, groups=self.groups)\n",
    "        self.resampling_cmplt = res\n",
    "        self.cmplt_pipeline = res.best_estimator_\n",
    "        self.cmplt_model = res.best_estimator_.named_steps['model']\n",
    "        return self.cmplt_model, res\n",
    "\n",
    "\n",
    "    def save_results(self, path, fname, model=None, cv_results=None, pipe=None):\n",
    "        \"\"\"\n",
    "        Saves the model, cross-validation results, and pipeline to the specified directories.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path to save the results.\n",
    "            fname (str): File name for saving the results.\n",
    "            model (optional): Trained model to save as a .pth file.\n",
    "            cv_results (optional): Cross-validation results to save as a .csv file.\n",
    "            pipe (optional): Pipeline to save as a pickle file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            logger.warning(\"Won't save any model, since its not provided\")\n",
    "        else:\n",
    "            model_dir = os.path.join(path, 'model')\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.model.to(torch.device('cpu'))\n",
    "            torch.save(model.model, os.path.join(model_dir, f\"{fname}.pth\"))\n",
    "            logger.info(f\"Saved model to {model_dir}\")\n",
    "\n",
    "        if cv_results is None:\n",
    "            logger.warning(\"Won't save any cv results, since its not provided\")\n",
    "        else:\n",
    "            results_dir = os.path.join(path, 'results')\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            results_file = os.path.join(results_dir, f\"{fname}_cv.csv\")\n",
    "            pd.DataFrame(cv_results).to_csv(results_file)\n",
    "            logger.info(f\"Saved CV results to {results_file}\")\n",
    "\n",
    "\n",
    "    def _check_modelling_prerequs(self, pipeline_steps):\n",
    "        \"\"\"\n",
    "        Checks whether the necessary prerequisites for the modeling process are met (data is prepared + model exists in pipeline).\n",
    "\n",
    "        Args:\n",
    "            pipeline_steps (list): List of (name, transformer) tuples representing the steps in the pipeline.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A boolean if error exists.\n",
    "        \"\"\"\n",
    "        err = False\n",
    "        mes = \"\"\n",
    "        if self.X is None or self.y is None:\n",
    "            mes = mes + \"1) Please call prepare_data() with your preferred config or set X, y, and groups\"\n",
    "            err = True\n",
    "        if not any('model' in tup for tup in pipeline_steps):\n",
    "            mes = mes + \"2) Caution! Your pipeline must include a step named 'model' for the model\"\n",
    "            err = True\n",
    "        return err, mes\n",
    "\n",
    "\n",
    "    def _get_config_vals(self, config):\n",
    "        \"\"\"\n",
    "        Extracts config values from the provided modelling dictionary.\n",
    "\n",
    "        Args:\n",
    "            config (dict): Configuration dictionary with keys such as 'params_cv', 'monitor',\n",
    "                        'do_nested_resampling', and 'refit'.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following extracted values:\n",
    "                - param_grid (dict or None): Parameter grid for cross-validation.\n",
    "                - monitor (object or None): Optional monitor object for early stopping.\n",
    "                - do_nested_resampling (bool): Nested resampling should be performed?\n",
    "                - refit_hp_tuning (bool): Refit the model with hyperparameter tuning?\n",
    "        \"\"\"\n",
    "        if config.get(\"params_cv\", None) is None:\n",
    "            logger.warning(\"No param grid for (nested) resampling detected - will fit model with default HPs on complete data\")\n",
    "            return None, False, False, False\n",
    "        if config.get('monitor', None) is None:\n",
    "            logger.info(\"No additional monitoring detected\")\n",
    "        return config['params_cv'], config.get('monitor', None), config.get('do_nested_resampling', True), config.get('refit', True)\n",
    "\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def _set_seed(self, seed = 1234):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        global random_state\n",
    "        random_state = check_random_state(seed)\n"
   ],
   "metadata": {
    "id": "VF7inZhJB0Xb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import SelectFromModel, SelectorMixin\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin, _fit_context, clone, is_classifier\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable\n",
    "import torch.nn.functional as F\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\"\"\"\n",
    "The code in this chunk is based on the autoencoder implementation\n",
    "in this repository https://github.com/phcavelar/pathwayae.\n",
    "\"\"\"\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim:int,\n",
    "            hidden_dims:list[int],\n",
    "            output_dim:int,\n",
    "            nonlinearity:Callable,\n",
    "            dropout_rate:float=0.5,\n",
    "            bias:bool=True,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        in_dims = [input_dim] + hidden_dims\n",
    "        out_dims = hidden_dims + [output_dim]\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out, bias=bias) for d_in, d_out in zip(in_dims, out_dims)])\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.dropout(self.nonlinearity(layer(x)))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "    def layer_activations(self, x:torch.Tensor) -> list[torch.Tensor]:\n",
    "        # To allow for activation normalisation\n",
    "        activations = [x]\n",
    "        for layer in self.layers[:-1]:\n",
    "            activations.append(self.dropout(self.nonlinearity(layer(activations[-1]))))\n",
    "        return activations[1:] + [self.layers[-1](activations[-1])]\n",
    "\n",
    "class NopLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "\n",
    "    def update_temperature(self,*args,**kwargs) -> None:\n",
    "        pass\n",
    "\n",
    "    def layer_activations(self,*args,**kwargs) -> list[torch.Tensor]:\n",
    "        return []\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim:int=None,\n",
    "            hidden_dims:list[int]=[128],\n",
    "            encoding_dim:int=64,\n",
    "            nonlinearity=F.relu,\n",
    "            final_nonlinearity=lambda x:x,\n",
    "            dropout_rate:float=0.5,\n",
    "            bias:bool=True,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        if input_dim is None:\n",
    "            raise ValueError(\"Must specify input dimension before initialising the model\")\n",
    "        try:\n",
    "            len(hidden_dims)\n",
    "        except TypeError:\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        self.encoder = MLP(input_dim, hidden_dims, encoding_dim, nonlinearity, dropout_rate, bias)\n",
    "        self.decoder = MLP(encoding_dim, hidden_dims[-1::-1], input_dim, nonlinearity, dropout_rate, bias)\n",
    "        self.final_nonlinearity = final_nonlinearity\n",
    "\n",
    "    def encode(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.final_nonlinearity(self.decoder(x))\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat\n",
    "\n",
    "    def layer_activations(self,x:torch.Tensor) -> list[torch.Tensor]:\n",
    "        # To allow for activation normalisation\n",
    "        encoder_activations = self.encoder.layer_activations(x)\n",
    "        decoder_activations = self.decoder.layer_activations(encoder_activations[-1])\n",
    "        return encoder_activations + decoder_activations\n",
    "\n",
    "    def get_feature_importance_matrix(self) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            feature_importance_matrix = self.encoder.layers[0].weight.T\n",
    "            for layer in self.encoder.layers[1:]:\n",
    "                feature_importance_matrix = torch.matmul(feature_importance_matrix, layer.weight.T)\n",
    "        return feature_importance_matrix.detach()"
   ],
   "metadata": {
    "id": "e4HG9NHLMFQN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FoldAwareAE(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer class, based on the transformer class from scikit-learn API.\n",
    "    Integrates an Autoencoder model for feature transformation and\n",
    "    loads a pretrained autoencoder model based on the cohorts present in the input dataset.\n",
    "\n",
    "    Attributes:\n",
    "        all_cohorts (list): List of all known cohort names.\n",
    "        model (Autoencoder): Instance of the autoencoder model.\n",
    "        testing (bool): Whether AE is used during testing or training.\n",
    "    \"\"\"\n",
    "    def __init__(self, testing = False):\n",
    "        self.all_cohorts = ['Atlanta_2014_Long', 'Belfast_2018_Jain', 'CamCap_2016_Ross_Adams',\n",
    "                            'CancerMap_2017_Luca', 'CPC_GENE_2017_Fraser', 'CPGEA_2020_Li',\n",
    "                            'DKFZ_2018_Gerhauser', 'MSKCC_2010_Taylor', 'Stockholm_2016_Ross_Adams']\n",
    "        self.model = None\n",
    "        self.testing = testing\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Dynamically loads a pretrained autoencoder model based on the cohorts present in the dataset.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): Input dataset with cohort info in the index.\n",
    "            y (ignored): Included for compatibility with scikit-learn.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted instance of the FoldAwareAE class.\n",
    "        \"\"\"\n",
    "        root = '/content'\n",
    "        if self.testing is False:\n",
    "            cohort_names = X.index.to_series().str.split('.').str[0]\n",
    "            unique_cohort_names = cohort_names.unique()\n",
    "            model_path = ''\n",
    "            for c in self.all_cohorts:\n",
    "                if c not in unique_cohort_names:\n",
    "                    if len(model_path) > 0:\n",
    "                        model_path +=  \"_\"\n",
    "                    model_path += c\n",
    "            if model_path == '':\n",
    "                model_path = 'pretrnd_cmplt'\n",
    "            model_path = os.path.join(root, model_path)\n",
    "        else:\n",
    "            model_path = 'pretrnd_cmplt'\n",
    "            model_path = os.path.join(root, model_path)\n",
    "\n",
    "        self.model = Autoencoder(input_dim=len(X.columns))\n",
    "\n",
    "        self.model.load_state_dict(torch.load(model_path + '.pth'))\n",
    "        self.model.eval()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Transforms the input data into its corresponding latent representation using the encoder.\n",
    "        Implemented to adhere to scikit-learn API.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): Input dataset to be transformed.\n",
    "            y (ignored): Included for compatibility with scikit-learn.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Latent representation of the input data with original index of X.\n",
    "        \"\"\"\n",
    "        X_t = torch.FloatTensor(X.values).to('cpu')\n",
    "        ls = self.model.encoder(X_t).detach().cpu().numpy()\n",
    "        ls = pd.DataFrame(ls, index=X.index)\n",
    "        return ls\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Combines the fit and transform steps. Implemented to adhere to scikit-learn API.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): Input dataset to be fitted and transformed.\n",
    "            y (ignored): Included for compatibility with scikit-learn.\n",
    "            **fit_params: Additional params for fit method.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: Latent representation of the input data after fitting + transforming.\n",
    "        \"\"\"\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)"
   ],
   "metadata": {
    "id": "t_vbX6vQ2uvK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# chunk 5\n",
    "DATA_CONFIG = {\n",
    "    'use_pca': False,\n",
    "    'pca_threshold': 0.85,\n",
    "    'gene_type': 'intersection',\n",
    "    'use_imputed': True,\n",
    "    'select_random' : False,\n",
    "    'use_cohorts': False,\n",
    "    'requires_ohenc' : True,\n",
    "    'only_pData': False,\n",
    "    'clinical_covs' : [\"AGE\", \"TISSUE\", \"GLEASON_SCORE\", 'PRE_OPERATIVE_PSA'] # remove if only Autoencoder without clinical data is to be trained\n",
    "}\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data(DATA_CONFIG)\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv'  : {\n",
    "        'model__hidden_layers': [[256, 128], [256]],\n",
    "        'model__learning_rate': [0.00001],\n",
    "        'model__batch_size': [64],\n",
    "        'model__num_epochs': [500],\n",
    "        'model__dropout': [0.2],\n",
    "        'model__device': ['cuda']\n",
    "    },\n",
    "    'refit': True,\n",
    "    'do_nested_resampling': True,\n",
    "    'path' : 'content',\n",
    "    'fname_cv' : 'deepsurv_autoencoder_pdata'\n",
    "}\n",
    "\n",
    "# If Autoencoder with clinical data is to be trained\n",
    "pdata_cols = ['TISSUE_FFPE','TISSUE_Fresh_frozen', 'TISSUE_Snap_frozen', 'AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA'] # remove if only Autoencoder without clinical data is to be trained\n",
    "exprs_cols =  list(set(mp.X.columns) - set(pdata_cols))\n",
    "exprs_cols = sorted(exprs_cols)\n",
    "\n",
    "# If  Autoencoder without clinical data is to be trained\n",
    "# exprs_cols = mp.X.columns\n",
    "# pdata_cols = []\n",
    "\n",
    "ae = FoldAwareAE()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('feature_selection', ae, exprs_cols),  # Apply feature selection\n",
    "        ('other_features', 'passthrough', pdata_cols)         # Pass through other columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipe_steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DeepSurvModel())]\n"
   ],
   "metadata": {
    "id": "a9Nk5zFMLMmq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mp.do_modelling(pipe_steps, MODEL_CONFIG)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70m1HUjlT_1y",
    "outputId": "7b521fc8-6c9a-4ed8-a09f-90d60ef8f21d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 117\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 20\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 88\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 61\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 103\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 116\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 32\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 101\n",
      "Fitting 8 folds for each of 2 candidates, totalling 16 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any model, since its not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 104\n",
      "Fitting 9 folds for each of 2 candidates, totalling 18 fits\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 117\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   5.6s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 88\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   4.4s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 61\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   3.5s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 104\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   4.8s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 46\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   2.3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.7s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 68\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=   6.5s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 104\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256, 128], model__learning_rate=1e-05, model__num_epochs=500; total time=  10.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 153\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   6.5s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 31\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.4s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 25\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 103\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   4.7s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 116\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   4.6s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   1.8s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 101\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   4.7s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n",
      "Early stopping at epoch 123\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[256], model__learning_rate=1e-05, model__num_epochs=500; total time=   5.3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-22-6aa95c1c05a7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path + '.pth'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "70\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any cv results, since its not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'mean_score': 0.6214761924447018,\n",
       "  'std_score': 0.0824873974682325,\n",
       "  'fold_results': [{'test_cohort': 'Atlanta_2014_Long',\n",
       "    'test_score': 0.6676300578034682,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([15.6319668 ,  7.31603545]),\n",
       "     'std_fit_time': array([6.53201347, 2.94259126]),\n",
       "     'mean_score_time': array([0.17088866, 0.08237451]),\n",
       "     'std_score_time': array([0.05691288, 0.0621777 ]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.51721708, 0.57712445]),\n",
       "     'split1_test_score': array([0.50935551, 0.54885655]),\n",
       "     'split2_test_score': array([0.67789541, 0.55598332]),\n",
       "     'split3_test_score': array([0.73815968, 0.48917456]),\n",
       "     'split4_test_score': array([0.50066649, 0.50466542]),\n",
       "     'split5_test_score': array([0.76455907, 0.75374376]),\n",
       "     'split6_test_score': array([0.48913043, 0.48951863]),\n",
       "     'split7_test_score': array([0.59838002, 0.59770503]),\n",
       "     'mean_test_score': array([0.59942046, 0.56459646]),\n",
       "     'std_test_score': array([0.10574974, 0.08083056]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}},\n",
       "   {'test_cohort': 'Belfast_2018_Jain',\n",
       "    'test_score': 0.5042287555376561,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([4.39266539, 4.29136607]),\n",
       "     'std_fit_time': array([1.9744556 , 1.68267415]),\n",
       "     'mean_score_time': array([0.08681017, 0.07924184]),\n",
       "     'std_score_time': array([0.02926384, 0.039272  ]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.44942197, 0.40953757]),\n",
       "     'split1_test_score': array([0.47817048, 0.40748441]),\n",
       "     'split2_test_score': array([0.5181264, 0.5874238]),\n",
       "     'split3_test_score': array([0.56021651, 0.60960758]),\n",
       "     'split4_test_score': array([0.59877366, 0.47720608]),\n",
       "     'split5_test_score': array([0.63144759, 0.64642263]),\n",
       "     'split6_test_score': array([0.43206522, 0.49534161]),\n",
       "     'split7_test_score': array([0.59028012, 0.46911914]),\n",
       "     'mean_test_score': array([0.53231274, 0.51276785]),\n",
       "     'std_test_score': array([0.06931513, 0.0851687 ]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}},\n",
       "   {'test_cohort': 'CPC_GENE_2017_Fraser',\n",
       "    'test_score': 0.46153846153846156,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([7.9156597 , 6.67319942]),\n",
       "     'std_fit_time': array([4.54373399, 2.57215022]),\n",
       "     'mean_score_time': array([0.09551898, 0.08757648]),\n",
       "     'std_score_time': array([0.02022254, 0.0467946 ]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.5916185 , 0.68323699]),\n",
       "     'split1_test_score': array([0.57561418, 0.46173983]),\n",
       "     'split2_test_score': array([0.61758101, 0.44145011]),\n",
       "     'split3_test_score': array([0.53856563, 0.60487145]),\n",
       "     'split4_test_score': array([0.68728339, 0.60063983]),\n",
       "     'split5_test_score': array([0.74292845, 0.42512479]),\n",
       "     'split6_test_score': array([0.62694099, 0.59860248]),\n",
       "     'split7_test_score': array([0.63820452, 0.54066824]),\n",
       "     'mean_test_score': array([0.62734208, 0.54454172]),\n",
       "     'std_test_score': array([0.06020778, 0.08710989]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}},\n",
       "   {'test_cohort': 'CPGEA_2020_Li',\n",
       "    'test_score': 0.6637792749438562,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([6.45628753, 5.56600699]),\n",
       "     'std_fit_time': array([2.84460589, 1.93952962]),\n",
       "     'mean_score_time': array([0.11151963, 0.07116014]),\n",
       "     'std_score_time': array([0.03936443, 0.04018366]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.58439306, 0.39537572]),\n",
       "     'split1_test_score': array([0.51057189, 0.5557793 ]),\n",
       "     'split2_test_score': array([0.53742204, 0.38045738]),\n",
       "     'split3_test_score': array([0.55277402, 0.63599459]),\n",
       "     'split4_test_score': array([0.59130898, 0.52892562]),\n",
       "     'split5_test_score': array([0.68801997, 0.56489185]),\n",
       "     'split6_test_score': array([0.59161491, 0.38315217]),\n",
       "     'split7_test_score': array([0.61187985, 0.439757  ]),\n",
       "     'mean_test_score': array([0.58349809, 0.4855417 ]),\n",
       "     'std_test_score': array([0.05030313, 0.09185619]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}},\n",
       "   {'test_cohort': 'CamCap_2016_Ross_Adams',\n",
       "    'test_score': 0.6407307171853857,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([7.09387159, 6.99115074]),\n",
       "     'std_fit_time': array([3.10513483, 2.70078616]),\n",
       "     'mean_score_time': array([0.08277911, 0.0990507 ]),\n",
       "     'std_score_time': array([0.0242296 , 0.03928124]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.67630058, 0.70751445]),\n",
       "     'split1_test_score': array([0.60954491, 0.58014499]),\n",
       "     'split2_test_score': array([0.42827443, 0.56237006]),\n",
       "     'split3_test_score': array([0.47321142, 0.56079564]),\n",
       "     'split4_test_score': array([0.56091709, 0.56784857]),\n",
       "     'split5_test_score': array([0.61148087, 0.6655574 ]),\n",
       "     'split6_test_score': array([0.55745342, 0.5613354 ]),\n",
       "     'split7_test_score': array([0.60580493, 0.49679379]),\n",
       "     'mean_test_score': array([0.56537345, 0.58779504]),\n",
       "     'std_test_score': array([0.07532356, 0.06244282]),\n",
       "     'rank_test_score': array([2, 1], dtype=int32)}},\n",
       "   {'test_cohort': 'CancerMap_2017_Luca',\n",
       "    'test_score': 0.6878165822447347,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([4.36032912, 6.48349285]),\n",
       "     'std_fit_time': array([1.63205618, 1.94581133]),\n",
       "     'mean_score_time': array([0.10197964, 0.0773311 ]),\n",
       "     'std_score_time': array([0.02439718, 0.03279349]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.37890173, 0.47283237]),\n",
       "     'split1_test_score': array([0.50060411, 0.59655658]),\n",
       "     'split2_test_score': array([0.61330561, 0.49480249]),\n",
       "     'split3_test_score': array([0.43182547, 0.533205  ]),\n",
       "     'split4_test_score': array([0.66170501, 0.64546685]),\n",
       "     'split5_test_score': array([0.5266223 , 0.78618968]),\n",
       "     'split6_test_score': array([0.57336957, 0.6238354 ]),\n",
       "     'split7_test_score': array([0.58184273, 0.53020587]),\n",
       "     'mean_test_score': array([0.53352207, 0.58538678]),\n",
       "     'std_test_score': array([0.08810493, 0.09498445]),\n",
       "     'rank_test_score': array([2, 1], dtype=int32)}},\n",
       "   {'test_cohort': 'DKFZ_2018_Gerhauser',\n",
       "    'test_score': 0.5856905158069884,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([6.55752403, 6.02408019]),\n",
       "     'std_fit_time': array([3.14322677, 1.85052956]),\n",
       "     'mean_score_time': array([0.09837785, 0.0714139 ]),\n",
       "     'std_score_time': array([0.0395139 , 0.03466455]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.6716763 , 0.49595376]),\n",
       "     'split1_test_score': array([0.63018526, 0.50191301]),\n",
       "     'split2_test_score': array([0.4022869 , 0.47401247]),\n",
       "     'split3_test_score': array([0.48091113, 0.52582611]),\n",
       "     'split4_test_score': array([0.64140731, 0.70771313]),\n",
       "     'split5_test_score': array([0.54012263, 0.57984537]),\n",
       "     'split6_test_score': array([0.58152174, 0.50349379]),\n",
       "     'split7_test_score': array([0.55248059, 0.5116436 ]),\n",
       "     'mean_test_score': array([0.56257398, 0.53755016]),\n",
       "     'std_test_score': array([0.08367722, 0.07048942]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}},\n",
       "   {'test_cohort': 'MSKCC_2010_Taylor',\n",
       "    'test_score': 0.7247670807453416,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([5.68618619, 6.99165568]),\n",
       "     'std_fit_time': array([1.88430513, 2.9689714 ]),\n",
       "     'mean_score_time': array([0.09899408, 0.08080241]),\n",
       "     'std_score_time': array([0.03942541, 0.03661757]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.51445087, 0.61416185]),\n",
       "     'split1_test_score': array([0.51560612, 0.51933145]),\n",
       "     'split2_test_score': array([0.31912682, 0.57588358]),\n",
       "     'split3_test_score': array([0.63875521, 0.5662496 ]),\n",
       "     'split4_test_score': array([0.47902571, 0.65290934]),\n",
       "     'split5_test_score': array([0.6206345 , 0.53532391]),\n",
       "     'split6_test_score': array([0.63144759, 0.43843594]),\n",
       "     'split7_test_score': array([0.54674317, 0.68444144]),\n",
       "     'mean_test_score': array([0.53322375, 0.57334214]),\n",
       "     'std_test_score': array([0.09877117, 0.07327963]),\n",
       "     'rank_test_score': array([2, 1], dtype=int32)}},\n",
       "   {'test_cohort': 'Stockholm_2016_Ross_Adams',\n",
       "    'test_score': 0.6571042861964226,\n",
       "    'best_params': {'model__batch_size': 64,\n",
       "     'model__device': 'cuda',\n",
       "     'model__dropout': 0.2,\n",
       "     'model__hidden_layers': [256, 128],\n",
       "     'model__learning_rate': 1e-05,\n",
       "     'model__num_epochs': 500},\n",
       "    'inner_cv_results': {'mean_fit_time': array([7.76771039, 6.35181442]),\n",
       "     'std_fit_time': array([3.37347141, 2.96738388]),\n",
       "     'mean_score_time': array([0.09322792, 0.0916875 ]),\n",
       "     'std_score_time': array([0.02992483, 0.05449637]),\n",
       "     'param_model__batch_size': masked_array(data=[64, 64],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'param_model__device': masked_array(data=['cuda', 'cuda'],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__dropout': masked_array(data=[0.2, 0.2],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__hidden_layers': masked_array(data=[list([256, 128]), list([256])],\n",
       "                  mask=[False, False],\n",
       "            fill_value='?',\n",
       "                 dtype=object),\n",
       "     'param_model__learning_rate': masked_array(data=[1e-05, 1e-05],\n",
       "                  mask=[False, False],\n",
       "            fill_value=1e+20),\n",
       "     'param_model__num_epochs': masked_array(data=[500, 500],\n",
       "                  mask=[False, False],\n",
       "            fill_value=999999),\n",
       "     'params': [{'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256, 128],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500},\n",
       "      {'model__batch_size': 64,\n",
       "       'model__device': 'cuda',\n",
       "       'model__dropout': 0.2,\n",
       "       'model__hidden_layers': [256],\n",
       "       'model__learning_rate': 1e-05,\n",
       "       'model__num_epochs': 500}],\n",
       "     'split0_test_score': array([0.70867052, 0.46184971]),\n",
       "     'split1_test_score': array([0.62645993, 0.50120822]),\n",
       "     'split2_test_score': array([0.42099792, 0.44282744]),\n",
       "     'split3_test_score': array([0.40680141, 0.57555342]),\n",
       "     'split4_test_score': array([0.67726658, 0.64343708]),\n",
       "     'split5_test_score': array([0.50439883, 0.43455079]),\n",
       "     'split6_test_score': array([0.69134775, 0.67720466]),\n",
       "     'split7_test_score': array([0.60830745, 0.57492236]),\n",
       "     'mean_test_score': array([0.5805313 , 0.53894421]),\n",
       "     'std_test_score': array([0.11311926, 0.08672756]),\n",
       "     'rank_test_score': array([1, 2], dtype=int32)}}]},\n",
       " DeepSurvModel(batch_size=64, device='cuda', dropout=0.2,\n",
       "               hidden_layers=[256, 128], learning_rate=1e-05, num_epochs=500),\n",
       " Pipeline(steps=[('preprocessor',\n",
       "                  ColumnTransformer(transformers=[('feature_selection',\n",
       "                                                   FoldAwareAE(),\n",
       "                                                   ['ENSG00000000003',\n",
       "                                                    'ENSG00000000005',\n",
       "                                                    'ENSG00000000419',\n",
       "                                                    'ENSG00000000457',\n",
       "                                                    'ENSG00000000460',\n",
       "                                                    'ENSG00000000938',\n",
       "                                                    'ENSG00000000971',\n",
       "                                                    'ENSG00000001036',\n",
       "                                                    'ENSG00000001084',\n",
       "                                                    'ENSG00000001167',\n",
       "                                                    'ENSG00000001461',\n",
       "                                                    'ENSG00000001497',\n",
       "                                                    'ENSG00000001561',\n",
       "                                                    'ENSG00000001617...\n",
       "                                                    'ENSG00000002919',\n",
       "                                                    'ENSG00000003056',\n",
       "                                                    'ENSG00000003137',\n",
       "                                                    'ENSG00000003147',\n",
       "                                                    'ENSG00000003249', ...]),\n",
       "                                                  ('other_features',\n",
       "                                                   'passthrough',\n",
       "                                                   ['TISSUE_FFPE',\n",
       "                                                    'TISSUE_Fresh_frozen',\n",
       "                                                    'TISSUE_Snap_frozen', 'AGE',\n",
       "                                                    'GLEASON_SCORE',\n",
       "                                                    'PRE_OPERATIVE_PSA'])])),\n",
       "                 ('model',\n",
       "                  DeepSurvModel(batch_size=64, device='cuda', dropout=0.2,\n",
       "                                hidden_layers=[256, 128], learning_rate=1e-05,\n",
       "                                num_epochs=500))]))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Process only the expression data using the autoencoder\n",
    "exprs_data = pd.read_csv('/content/exprs_intersect.csv', index_col=0)\n",
    "ae = FoldAwareAE()\n",
    "ae.fit(exprs_data)\n",
    "exprs_encoded = ae.transform(exprs_data)\n",
    "\n",
    "# 2. Prepare clinical data\n",
    "pdata = pd.read_csv('/content/merged_imputed_pData.csv', index_col=0)\n",
    "clin_data = pdata.loc[:, DATA_CONFIG['clinical_covs']]\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "cat_cols = clin_data.select_dtypes(exclude=['number']).columns\n",
    "num_cols = clin_data.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Apply one-hot encoding for categorical data if required\n",
    "if DATA_CONFIG.get('requires_ohenc', True):\n",
    "    ohc = OneHotEncoder()\n",
    "    clin_data_cat = ohc.fit_transform(clin_data[cat_cols])\n",
    "    clin_data_cat = pd.DataFrame.sparse.from_spmatrix(\n",
    "        clin_data_cat,\n",
    "        columns=ohc.get_feature_names_out(),\n",
    "        index=exprs_data.index\n",
    "    )\n",
    "\n",
    "clin_data_num = clin_data[num_cols]\n",
    "\n",
    "# 3. Combine encoded expression data with clinical data\n",
    "X_combined = pd.concat([clin_data_cat, clin_data_num, exprs_encoded], axis=1)\n",
    "\n",
    "# 4. Train the DeepSurv model with the combined data\n",
    "model_params = {k.replace('model__', ''): v[0] for k, v in MODEL_CONFIG['params_cv'].items()}\n",
    "model = DeepSurvModel(**model_params)\n",
    "model.fit(X_combined, mp.y)\n"
   ],
   "metadata": {
    "id": "2XoZgw3d3NPQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Define configurations\n",
    "DATA_CONFIG = {\n",
    "    'use_pca': False,\n",
    "    'gene_type': 'intersection',\n",
    "    'use_imputed': True,\n",
    "    'use_cohorts': False,\n",
    "    'requires_ohenc': True,\n",
    "    'only_pData': False,\n",
    "    'clinical_covs' : [\"AGE\", \"TISSUE\", \"GLEASON_SCORE\", 'PRE_OPERATIVE_PSA']\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv': {\n",
    "        'model__hidden_layers': [[256, 128]],\n",
    "        'model__learning_rate': [0.00001],\n",
    "        'model__batch_size': [64],\n",
    "        'model__num_epochs': [500],\n",
    "        'model__dropout': [0.2],\n",
    "        'model__device': ['cuda']\n",
    "    },\n",
    "    'refit': False,\n",
    "    'do_nested_resampling': False,\n",
    "    'path': '/content/saved_model',\n",
    "    'fname_cv': 'deepsurv_ae_final'\n",
    "}\n",
    "\n",
    "def breslow_baseline_hazard(model, X, times, events):\n",
    "    \"\"\"\n",
    "    Computes the Breslow estimator for the cumulative baseline hazard\n",
    "    \"\"\"\n",
    "    log_risk = model.predict(X)\n",
    "    risk = np.exp(log_risk)\n",
    "    risk = np.clip(risk, 1e-10, None)  # Ensure numerical stability\n",
    "\n",
    "    order = np.argsort(times)\n",
    "    sorted_times = times[order]\n",
    "    sorted_events = events[order]\n",
    "    sorted_risk = risk[order]\n",
    "\n",
    "    unique_event_times = np.unique(sorted_times[sorted_events == 1])\n",
    "\n",
    "    bhaz = []\n",
    "    at_risk_sum = np.zeros_like(unique_event_times)\n",
    "    event_count = np.zeros_like(unique_event_times)\n",
    "\n",
    "    for i, t in enumerate(unique_event_times):\n",
    "        at_risk = sorted_risk[sorted_times >= t]\n",
    "        at_risk_sum[i] = at_risk.sum()\n",
    "        event_count[i] = np.sum((sorted_times == t) & (sorted_events == 1))\n",
    "\n",
    "    bhaz = event_count / np.maximum(at_risk_sum, 1e-8)\n",
    "    cbhaz = np.cumsum(bhaz)\n",
    "\n",
    "    return unique_event_times, bhaz, cbhaz\n",
    "\n",
    "def load_and_predict_survival(X_test, save_dir=\"/content/saved_model\"):\n",
    "    \"\"\"\n",
    "    Load the model and make survival predictions\n",
    "    \"\"\"\n",
    "    clin_data = X_test.iloc[:, :6]\n",
    "    exprs_data = X_test.iloc[:, 6:]\n",
    "\n",
    "    print(\"Clinical Features:\", clin_data.shape[1])\n",
    "    print(\"Expression Features:\", exprs_data.shape[1])\n",
    "\n",
    "    model_state = torch.load(os.path.join(save_dir, \"deep_surv_ae_state.pth\"))\n",
    "    model = DeepSurvModel(**model_state['model_params'])\n",
    "    model.init_network(70)\n",
    "    model.model.load_state_dict(model_state['model_state'])\n",
    "    model.model.eval()\n",
    "\n",
    "    ae = FoldAwareAE()\n",
    "    ae.model = Autoencoder(input_dim=13214)\n",
    "    ae.model.load_state_dict(torch.load(os.path.join(save_dir, \"autoencoder_state.pth\")))\n",
    "    ae.model.eval()\n",
    "\n",
    "    X_encoded = ae.transform(exprs_data)\n",
    "    X_combined = pd.concat([clin_data, X_encoded], axis=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_risk = model.predict(X_combined)\n",
    "        risk = np.exp(log_risk)\n",
    "        risk = np.clip(risk, 1e-10, None)\n",
    "\n",
    "    times = model_state['unique_event_times']\n",
    "    cbhaz = model_state['cum_baseline_hazard']\n",
    "\n",
    "    surv_list = [np.exp(-lam_0_t * risk) for lam_0_t in cbhaz]\n",
    "    surv = np.vstack(surv_list).T\n",
    "    mean_surv = np.mean(surv, axis=0)\n",
    "\n",
    "    return times, mean_surv, surv\n",
    "\n",
    "# 2. Prepare data\n",
    "exprs_data = pd.read_csv('/content/exprs_intersect.csv', index_col=0)\n",
    "ae = FoldAwareAE()\n",
    "ae.fit(exprs_data)\n",
    "exprs_encoded = ae.transform(exprs_data)\n",
    "\n",
    "pdata = pd.read_csv('/content/merged_imputed_pData.csv', index_col=0)\n",
    "clin_data = pdata[DATA_CONFIG['clinical_covs']]\n",
    "\n",
    "cat_cols = clin_data.select_dtypes(exclude=['number']).columns\n",
    "num_cols = clin_data.select_dtypes(exclude=['object']).columns\n",
    "if DATA_CONFIG.get('requires_ohenc', True):\n",
    "    ohc = OneHotEncoder()\n",
    "    clin_data_cat = ohc.fit_transform(clin_data[cat_cols])\n",
    "    clin_data_cat = pd.DataFrame.sparse.from_spmatrix(\n",
    "        clin_data_cat,\n",
    "        columns=ohc.get_feature_names_out(),\n",
    "        index=exprs_data.index\n",
    "    )\n",
    "clin_data_num = clin_data[num_cols]\n",
    "clin_data_processed = pd.concat([clin_data_cat, clin_data_num], axis=1)\n",
    "\n",
    "X_combined = pd.concat([clin_data_processed, exprs_encoded], axis=1)\n",
    "print(\"Final feature dimensions:\", X_combined.shape[1])\n",
    "\n",
    "status = pdata['BCR_STATUS'].astype(bool).values\n",
    "time = pdata['MONTH_TO_BCR'].astype(float).values\n",
    "y = Surv.from_arrays(event=status, time=time, name_event='status', name_time='time')\n",
    "\n",
    "model_params = {k.replace('model__', ''): v[0] for k, v in MODEL_CONFIG['params_cv'].items()}\n",
    "model = DeepSurvModel(**model_params)\n",
    "model.fit(X_combined, y)\n",
    "\n",
    "times, bhaz, cbhaz = breslow_baseline_hazard(model=model, X=X_combined, times=y['time'], events=y['status'])\n",
    "\n",
    "save_dir = \"/content/saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_state = {\n",
    "    'model_params': model_params,\n",
    "    'model_state': model.model.state_dict(),\n",
    "    'unique_event_times': times,\n",
    "    'cum_baseline_hazard': cbhaz\n",
    "}\n",
    "\n",
    "torch.save(model_state, os.path.join(save_dir, \"deep_surv_ae_state.pth\"))\n",
    "torch.save(ae.model.state_dict(), os.path.join(save_dir, \"autoencoder_state.pth\"))\n",
    "print(\"Model saved.\")\n",
    "\n",
    "X_complete = pd.concat([clin_data_processed, exprs_data], axis=1)\n",
    "times, mean_surv, all_surv = load_and_predict_survival(X_complete)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.step(times, mean_surv, where='post', color='blue', label='Mean Survival')\n",
    "plt.fill_between(times, np.percentile(all_surv, 25, axis=0), np.percentile(all_surv, 75, axis=0), alpha=0.2, step='post', color='blue')\n",
    "plt.xlabel('Time (months)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Predicted Survival Curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "4oludV5HDQDH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_survival_curves(exprs_test, pdata_test, save_dir=\"/content/saved_model\"):\n",
    "    \"\"\"\n",
    "    Makes survival predictions for a test dataset\n",
    "    \"\"\"\n",
    "    # 1. Prepare clinical data\n",
    "    clin_data = pdata_test[DATA_CONFIG['clinical_covs']]\n",
    "\n",
    "    # Separate categorical and numerical columns\n",
    "    cat_cols = ['TISSUE']  # Only TISSUE is categorical\n",
    "    num_cols = ['AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "\n",
    "    # Manual one-hot encoding for TISSUE\n",
    "    clin_data_cat = pd.DataFrame(index=clin_data.index)\n",
    "    clin_data_cat['TISSUE_FFPE'] = 0\n",
    "    clin_data_cat['TISSUE_Fresh_frozen'] = 0\n",
    "    clin_data_cat['TISSUE_Snap_frozen'] = 0\n",
    "\n",
    "    # Set the appropriate column to 1 based on TISSUE value\n",
    "    for idx, tissue in clin_data['TISSUE'].items():\n",
    "        col_name = f'TISSUE_{tissue}'\n",
    "        if col_name in clin_data_cat.columns:\n",
    "            clin_data_cat.loc[idx, col_name] = 1\n",
    "\n",
    "    # Numerical data\n",
    "    clin_data_num = clin_data[num_cols]\n",
    "\n",
    "    # Combine categorical and numerical data\n",
    "    clin_data_processed = pd.concat([clin_data_cat, clin_data_num], axis=1)\n",
    "\n",
    "    # 2. Load model states\n",
    "    model_state = torch.load(os.path.join(save_dir, \"deep_surv_ae_state.pth\"))\n",
    "\n",
    "    # 3. Initialize and load DeepSurv model\n",
    "    model = DeepSurvModel(**model_state['model_params'])\n",
    "    input_dim = 64 + clin_data_processed.shape[1]  # AE output + clinical features\n",
    "    model.init_network(input_dim)\n",
    "    model.model.load_state_dict(model_state['model_state'])\n",
    "    model.model.eval()\n",
    "\n",
    "    # 4. Load and apply the autoencoder\n",
    "    ae = FoldAwareAE()\n",
    "    ae.model = Autoencoder(input_dim=exprs_test.shape[1])\n",
    "    ae.model.load_state_dict(torch.load(os.path.join(save_dir, \"autoencoder_state.pth\")))\n",
    "    ae.model.eval()\n",
    "\n",
    "    # 5. Transform expression data using the autoencoder\n",
    "    exprs_encoded = ae.transform(exprs_test)\n",
    "\n",
    "    # 6. Combine encoded expression features with clinical features\n",
    "    X_combined = pd.concat([clin_data_processed, exprs_encoded], axis=1)\n",
    "\n",
    "    # 7. Compute risk score\n",
    "    with torch.no_grad():\n",
    "        log_risk = model.predict(X_combined)\n",
    "        risk = np.exp(log_risk)\n",
    "        risk = np.clip(risk, 1e-10, None)\n",
    "\n",
    "    # 8. Compute survival function\n",
    "    times = model_state['unique_event_times']\n",
    "    cbhaz = model_state['cum_baseline_hazard']\n",
    "\n",
    "    # Compute survival for each patient\n",
    "    surv_curves = np.array([np.exp(-cbhaz * r) for r in risk])\n",
    "\n",
    "    # 9. Create DataFrame\n",
    "    result_df = pd.DataFrame({'time': times})\n",
    "\n",
    "    # Add a column for each patient\n",
    "    for i, patient_id in enumerate(exprs_test.index):\n",
    "        result_df[f'patient_{patient_id}'] = surv_curves[i]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Load new test data\n",
    "exprs_test = pd.read_csv('/content/intersect_genes_test_cohort1_low_risk.csv', index_col=0)\n",
    "pdata_test = pd.read_csv('/content/low_risk_pData_test_cohort1.csv', index_col=0)\n",
    "\n",
    "# Make predictions\n",
    "survival_curves = predict_survival_curves(exprs_test, pdata_test)\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"/content/survival_data\", exist_ok=True)\n",
    "survival_curves.to_csv(\"/content/survival_data/predicted_survival_curves.csv\", index=False)\n",
    "\n",
    "# Plot survival curves for all patients\n",
    "plt.figure(figsize=(12,8))\n",
    "for patient in survival_curves.columns[1:]:  # First column is 'time'\n",
    "    plt.step(survival_curves['time'], survival_curves[patient],\n",
    "             where='post', alpha=0.3, label=patient)\n",
    "\n",
    "plt.xlabel('Time (months)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Predicted Survival Curves for All Patients')\n",
    "plt.grid(True)\n",
    "if len(survival_curves.columns) <= 11:\n",
    "    plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Xuu_lG_BDZo6"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
