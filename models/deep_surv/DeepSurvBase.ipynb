{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# How to use this code\n",
    "## !Use Google Colab:\n",
    "https://colab.research.google.com/drive/1cSgfV7vvcuAO7hVk3x-_dDB_3lQp9Rvr\n",
    "1. Run chunk 1\n",
    "2. Define the correct file path for the training data in Chunk 2 + run chunk 2\n",
    "3. Run chunk 3\n",
    "\n",
    "### To perform neste resampling:\n",
    "Adapt params in chunk 4 and run it\n",
    "This stores the best model as a pth file which can be ignored\n",
    "### To train model given parameters and save model as .pkl:\n",
    "Adapt paramt in chunk 5 as explained and then run it. The .pkl file is requiered for getting predictions on test data.\n",
    "### To get model predictions on test data:\n",
    "In chunk 6 define the path to the .pkl model file, as well as to expression and pData test data sets. Then run the chunk."
   ],
   "metadata": {
    "id": "mWfTsSQwC6YK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jd6_BoqFQ7Zg",
    "outputId": "98878c1f-43da-442c-ae81-b98c806fdc21"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting lifelines\n",
      "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.13.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.10.0)\n",
      "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.7.0)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting formulaic>=0.2.2 (from lifelines)\n",
      "  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\n",
      "Downloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m349.3/349.3 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.7/115.7 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4031 sha256=c9618d2d9ebf95e6943982870fd2d4f551a712494d8dab4a36693b7378afbcc0\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: interface-meta, autograd-gamma, formulaic, lifelines\n",
      "Successfully installed autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.3/13.3 MB\u001B[0m \u001B[31m71.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed scikit-learn-1.5.2\n",
      "Collecting scikit-survival==0.23.1\n",
      "  Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.0/49.0 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ecos (from scikit-survival==0.23.1)\n",
      "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.4.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.26.4)\n",
      "Requirement already satisfied: osqp!=0.6.0,!=0.6.1 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (0.6.7.post3)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<1.6,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from scikit-survival==0.23.1) (1.5.2)\n",
      "Requirement already satisfied: qdldl in /usr/local/lib/python3.11/dist-packages (from osqp!=0.6.0,!=0.6.1->scikit-survival==0.23.1) (0.1.7.post5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->scikit-survival==0.23.1) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6,>=1.4.0->scikit-survival==0.23.1) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->scikit-survival==0.23.1) (1.17.0)\n",
      "Downloading scikit_survival-0.23.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/3.9 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m220.1/220.1 kB\u001B[0m \u001B[31m14.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ecos, scikit-survival\n",
      "Successfully installed ecos-2.0.14 scikit-survival-0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lifelines\n",
    "!pip install scikit-learn==1.5.2\n",
    "!pip install scikit-survival==0.23.1\n",
    "# Required imports\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_random_state\n",
    "from sksurv.util import Surv\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 2\n",
    "# Defining the pathways to the data used for model training.\n",
    "# one pData file is needed.\n",
    "# /content is the folder which serves as the standard upload folder in google colab\n",
    "CLINICAL_DATA_PATH = '/content/merged_imputed_pData.csv'"
   ],
   "metadata": {
    "id": "cnTLQHvADIB1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 3\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DeepSurvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch based neural network architecture designed for survival prediction.\n",
    "    This network consists of fully connected layers with ReLU activation,\n",
    "    dropout for regularization, and a final layer that outputs a single\n",
    "    hazard prediction value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, hidden_layers=[32, 16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = n_features\n",
    "        self.model = None\n",
    "\n",
    "        # Create network layers\n",
    "        for size in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_size = size\n",
    "\n",
    "        # Hazard output layer\n",
    "        layers.append(nn.Linear(prev_size, 1, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DeepSurvModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Implementation of the DeepSurv model that integrates\n",
    "    with scikit-learn, specifying  configurable architecture,\n",
    "    training procedures, and evaluation metrics.\n",
    "\n",
    "    The model includes:\n",
    "    - Customizable neural network architecture\n",
    "    - Mini-batch training with early stopping\n",
    "    - CPU/GPU support\n",
    "    - Concordance index evaluation\n",
    "    - Compatibility with scikit-learn's cross-validation and pipeline features\n",
    "    - Reproducible training through seed control\n",
    "\n",
    "    The model follows scikit-learn's estimator interface by implementing\n",
    "    fit(), predict(), get_params() and set_params() methods.\n",
    "    \"\"\"\n",
    "    # Main survival model class\n",
    "    def __init__(self, n_features=None, hidden_layers=[16, 16], dropout=0.5,\n",
    "                 learning_rate=0.01, device='cpu', random_state=123,\n",
    "                 batch_size=128, num_epochs=100, patience=15):\n",
    "        self.n_features = n_features\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = device if torch.cuda.is_available() and device == 'cuda' else 'cpu'\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.is_fitted_ = False\n",
    "        self.training_history_ = {'train_loss': [], 'val_loss': []}\n",
    "        self.n_features_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Data validation\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        # Set seeds\n",
    "        np.random.seed(self.random_state)\n",
    "        torch.manual_seed(self.random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(self.random_state)\n",
    "            torch.cuda.manual_seed_all(self.random_state)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.init_network(self.n_features_in_)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        train_loader, val_loader = self._prepare_data(X, y, val_split=0.1)\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        counter = 0.0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss_ = 0.0\n",
    "            n_batches_ = 0\n",
    "            for X_batch, time_batch, event_batch in train_loader:\n",
    "                loss = self._train_step(X_batch, time_batch, event_batch)\n",
    "                epoch_loss_ += loss\n",
    "                n_batches_ += 1\n",
    "            avg_train_loss = epoch_loss_ / n_batches_\n",
    "            self.training_history_['train_loss'].append(avg_train_loss)\n",
    "\n",
    "            # Validate\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, time_batch, event_batch in val_loader:\n",
    "                    val_loss += self._eval_step(X_batch, time_batch, event_batch)\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            self.training_history_['val_loss'].append(val_loss)\n",
    "\n",
    "            # Model checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter > self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        X = torch.FloatTensor(X).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            risk_scores = self.model(X).cpu().numpy()\n",
    "        return risk_scores.flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        preds = self.predict(X)\n",
    "        return self.c_index(-preds, y)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"n_features\": self.n_features,\n",
    "            \"hidden_layers\": self.hidden_layers,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"device\": self.device,\n",
    "            \"random_state\": self.random_state,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_epochs\": self.num_epochs,\n",
    "            \"patience\": self.patience\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def clone(self):\n",
    "        super(self).clone()\n",
    "\n",
    "    def _prepare_data(self, X, y, val_split = 0.1):\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, random_state=self.random_state)\n",
    "\n",
    "        # Process training data\n",
    "        X_scaled_train = X_train\n",
    "        times_train = np.ascontiguousarray(y_train['time']).astype(np.float32)\n",
    "        event_field_train = 'status' if 'status' in y_train.dtype.names else 'event'\n",
    "        events_train = np.ascontiguousarray(y_train[event_field_train]).astype(np.float32)\n",
    "        X_tensor_train = torch.FloatTensor(X_scaled_train).to(self.device)\n",
    "        time_tensor_train = torch.FloatTensor(times_train).to(self.device)\n",
    "        event_tensor_train = torch.FloatTensor(events_train).to(self.device)\n",
    "\n",
    "        # Process validation data\n",
    "        X_scaled_val = X_val\n",
    "        times_val = np.ascontiguousarray(y_val['time']).astype(np.float32)\n",
    "        event_field_val = 'status' if 'status' in y_val.dtype.names else 'event'\n",
    "        events_val = np.ascontiguousarray(y_val[event_field_val]).astype(np.float32)\n",
    "        X_tensor_val = torch.FloatTensor(X_scaled_val).to(self.device)\n",
    "        time_tensor_val = torch.FloatTensor(times_val).to(self.device)\n",
    "        event_tensor_val = torch.FloatTensor(events_val).to(self.device)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_dataset = TensorDataset(X_tensor_train, time_tensor_train, event_tensor_train)\n",
    "        val_dataset = TensorDataset(X_tensor_val, time_tensor_val, event_tensor_val)\n",
    "\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(self.random_state)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            generator=generator\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def _negative_log_likelihood(self, risk_pred, times, events):\n",
    "        # Loss calculation\n",
    "        _, idx = torch.sort(times, descending=True)\n",
    "        risk_pred = risk_pred[idx]\n",
    "        events = events[idx]\n",
    "        log_risk = risk_pred\n",
    "        risk = torch.exp(log_risk)\n",
    "        cumsum_risk = torch.cumsum(risk, dim=0)\n",
    "        log_cumsum_risk = torch.log(cumsum_risk + 1e-10)\n",
    "        event_loss = events * (log_risk - log_cumsum_risk)\n",
    "        return -torch.mean(event_loss)\n",
    "\n",
    "    def _train_step(self, X, times, events):\n",
    "        # Single training iteration\n",
    "        self.optimizer.zero_grad()\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _eval_step(self, X, times, events):\n",
    "        # Single evaluation iteration\n",
    "        risk_pred = self.model(X)\n",
    "        loss = self._negative_log_likelihood(risk_pred, times, events)\n",
    "        return loss.item()\n",
    "\n",
    "    def c_index(self, risk_pred, y):\n",
    "        # Calculate concordance index\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = y.detach().cpu().numpy()\n",
    "        event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "        time = y['time']\n",
    "        event = y[event_field]\n",
    "        if not isinstance(risk_pred, np.ndarray):\n",
    "            risk_pred = risk_pred.detach().cpu().numpy()\n",
    "        if np.isnan(risk_pred).all():\n",
    "            return np.nan\n",
    "        return concordance_index(time, risk_pred, event)\n",
    "\n",
    "    def init_network(self, n_features):\n",
    "        # Initialize model and optimizer\n",
    "        self.model = DeepSurvNet(n_features=n_features, hidden_layers=self.hidden_layers, dropout=self.dropout).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "        # Helper functions for data handling\n",
    "def _get_survival_subset(y, indices):\n",
    "    \"\"\"Extract survival data subset\"\"\"\n",
    "    subset = np.empty(len(indices), dtype=y.dtype)\n",
    "    event_field = 'status' if 'status' in y.dtype.names else 'event'\n",
    "    subset[event_field] = y[event_field][indices]\n",
    "    subset['time'] = y['time'][indices]\n",
    "    return subset\n",
    "\n",
    "def _aggregate_results(results):\n",
    "    \"\"\"Process CV results\"\"\"\n",
    "    scores = [res['test_score'] for res in results]\n",
    "    if np.isnan(scores).all():\n",
    "        logger.warning(f\"Found only NaN values in CV-results: {scores}\")\n",
    "        mean_score, std_score = np.nan, np.nan\n",
    "    else:\n",
    "        mean_score = np.nanmean(scores)\n",
    "        std_score = np.nanstd(scores)\n",
    "\n",
    "    logger.info(f\"Aggregated results:\")\n",
    "    logger.info(f\"Mean score: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "    logger.info(f\"Individual scores: {scores}\")\n",
    "\n",
    "    return {\n",
    "        'mean_score': mean_score,\n",
    "        'std_score': std_score,\n",
    "        'fold_results': results\n",
    "    }\n",
    "\n",
    "def nested_resampling(estimator, X, y, groups, param_grid, monitor = None, ss = GridSearchCV,\n",
    "                     outer_cv = LeaveOneGroupOut(), inner_cv = LeaveOneGroupOut(), scoring = None):\n",
    "    \"\"\"Implementation of the nested resampling logic for hyperparameter optimization\"\"\"\n",
    "    # Main CV implementation\n",
    "    logger.info(\"Starting nested resampling...\")\n",
    "    logger.info(f\"Data shape: X={X.shape}, groups={len(np.unique(groups))} unique\")\n",
    "\n",
    "    outer_results = []\n",
    "    splits = list(outer_cv.split(X, y, groups))\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        logger.info(f\"\\nOuter fold {i+1}\")\n",
    "\n",
    "        fold_seed = 42 + i\n",
    "        np.random.seed(fold_seed)\n",
    "        torch.manual_seed(fold_seed)\n",
    "\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = _get_survival_subset(y, train_idx)\n",
    "        y_test = _get_survival_subset(y, test_idx)\n",
    "        train_groups = groups[train_idx] if groups is not None else None\n",
    "\n",
    "        test_cohort = groups[test_idx][0] if groups is not None else None\n",
    "        logger.info(f\"Test cohort: {test_cohort}\")\n",
    "\n",
    "        inner_gcv = ss(estimator, param_grid, cv = inner_cv, refit = True, n_jobs=1, verbose = 2)\n",
    "        if monitor is not None:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups, model__monitor = monitor)\n",
    "        else:\n",
    "            inner_results = inner_gcv.fit(X_train, y_train, groups = train_groups)\n",
    "\n",
    "        inner_cv_results = inner_results.cv_results_\n",
    "        inner_best_params = inner_results.best_params_\n",
    "\n",
    "        outer_model = inner_results.best_estimator_.named_steps['model']\n",
    "        test_score = outer_model.score(X_test, y_test)\n",
    "\n",
    "        logger.info(f\"Best parameters: {inner_best_params}\")\n",
    "        logger.info(f\"Test score: {test_score:.3f}\")\n",
    "\n",
    "        outer_results.append({\n",
    "            'test_cohort': test_cohort,\n",
    "            'test_score': test_score,\n",
    "            'best_params': inner_best_params,\n",
    "            'inner_cv_results': inner_cv_results\n",
    "        })\n",
    "\n",
    "    return _aggregate_results(outer_results)\n",
    "\n",
    "class ModellingProcess():\n",
    "    \"\"\"\n",
    "    This class manages the entire modeling process including data preparation,\n",
    "    nested cross-validation, model training, and result saving. It is a\n",
    "    standardized way of modeling used for several of the implemented mode types\n",
    "    and supports both simple training and complex nested resampling approaches.\n",
    "    Results can be automatically saved and evaluated.\n",
    "    \"\"\"\n",
    "    # Main modeling workflow class\n",
    "    def __init__(self) -> None:\n",
    "        self.outer_cv = LeaveOneGroupOut()\n",
    "        self.inner_cv = LeaveOneGroupOut()\n",
    "        self.ss = GridSearchCV\n",
    "        self.pipe = None\n",
    "        self.cmplt_model = None\n",
    "        self.cmplt_pipeline = None\n",
    "        self.nrs = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.groups = None\n",
    "        self.path = None\n",
    "        self.fname_cv = None\n",
    "\n",
    "    def prepare_survival_data(self, pdata):\n",
    "        # Format survival data\n",
    "        status = pdata['BCR_STATUS'].astype(bool).values\n",
    "        time = pdata['MONTH_TO_BCR'].astype(float).values\n",
    "        y = Surv.from_arrays(\n",
    "            event=status,\n",
    "            time=time,\n",
    "            name_event='status',\n",
    "            name_time='time'\n",
    "        )\n",
    "        return y\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Load and process data\n",
    "        pdata = pd.read_csv(CLINICAL_DATA_PATH, index_col=0)\n",
    "\n",
    "        feature_cols = ['AGE', 'TISSUE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "        target_cols = ['MONTH_TO_BCR', 'BCR_STATUS']\n",
    "\n",
    "        numeric_features = ['AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "        categorical_features = ['TISSUE']\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "            ])\n",
    "\n",
    "        X_raw = pdata[feature_cols]\n",
    "        self.X = pd.DataFrame(\n",
    "            preprocessor.fit_transform(X_raw),\n",
    "            index=X_raw.index\n",
    "        )\n",
    "\n",
    "        target_df = pdata[target_cols]\n",
    "        self.y = self.prepare_survival_data(target_df)\n",
    "\n",
    "        self.groups = np.array([idx.split('.')[0] for idx in self.X.index])\n",
    "\n",
    "    def do_modelling(self, pipeline_steps, config):\n",
    "        # Execute modeling pipeline\n",
    "        self._set_seed()\n",
    "\n",
    "        if config.get(\"params_mp\", None) is not None:\n",
    "            self.set_params(config['params_mp'])\n",
    "\n",
    "        if config.get(\"path\", None) is None or config.get(\"fname_cv\", None) is None:\n",
    "            logger.warning(\"Didn't get sufficient path info for saving cv-results\")\n",
    "        else:\n",
    "            self.path = config['path']\n",
    "            self.fname_cv = config['fname_cv']\n",
    "\n",
    "        err, mes = self._check_modelling_prerequs(pipeline_steps)\n",
    "        if err:\n",
    "            logger.error(\"Requirements setup error: %s\", mes)\n",
    "            raise Exception(mes)\n",
    "        else:\n",
    "            self.pipe = Pipeline(pipeline_steps)\n",
    "\n",
    "        param_grid, monitor, do_nested_resampling, refit_hp_tuning = self._get_config_vals(config)\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Start model training...\")\n",
    "            logger.info(f\"Input data shape: X={self.X.shape}\")\n",
    "\n",
    "            if do_nested_resampling:\n",
    "                logger.info(\"Nested resampling...\")\n",
    "                self.nrs = nested_resampling(self.pipe, self.X, self.y, self.groups, param_grid, monitor, self.ss, self.outer_cv, self.inner_cv)\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model=None, cv_results=self.nrs, pipe=None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during nested resampling: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        if refit_hp_tuning:\n",
    "            try:\n",
    "                logger.info(\"Do HP Tuning for complete model; refit + set complete model\")\n",
    "                self.fit_cmplt_model(param_grid)\n",
    "                if (self.fname_cv is not None) and (self.path is not None):\n",
    "                    self.save_results(self.path, self.fname_cv, model=self.cmplt_model, cv_results=None, pipe=self.cmplt_pipeline)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during complete model training: {str(e)}\")\n",
    "                raise\n",
    "        elif refit_hp_tuning is False and do_nested_resampling is False:\n",
    "            logger.info(\"Fit complete pipeline wo. HP tuning (on default params)\")\n",
    "            self.cmplt_pipeline = self.pipe.fit(self.X, self.y)\n",
    "            if (self.fname_cv is not None) and (self.path is not None):\n",
    "                self.save_results(self.path, self.fname_cv, model=None, cv_results=None, pipe=self.cmplt_pipeline)\n",
    "\n",
    "        return self.nrs, self.cmplt_model, self.cmplt_pipeline\n",
    "\n",
    "    def fit_cmplt_model(self, param_grid, monitor=None):\n",
    "        # Train final model\n",
    "        logger.info(\"Do HP Tuning for complete model\")\n",
    "        res = self.ss(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=self.outer_cv,\n",
    "            n_jobs=1,\n",
    "            verbose=2,\n",
    "            refit=True\n",
    "        )\n",
    "        if monitor is not None:\n",
    "            res.fit(self.X, self.y, groups=self.groups, model__monitor=monitor)\n",
    "        else:\n",
    "            res.fit(self.X, self.y, groups=self.groups)\n",
    "        self.resampling_cmplt = res\n",
    "        self.cmplt_pipeline = res.best_estimator_\n",
    "        self.cmplt_model = res.best_estimator_.named_steps['model']\n",
    "        return self.cmplt_model, res\n",
    "\n",
    "    def save_results(self, path, fname, model=None, cv_results=None, pipe=None):\n",
    "        # Save model and results\n",
    "        if model is None:\n",
    "            logger.warning(\"Won't save any model, since it's not provided\")\n",
    "        else:\n",
    "            # Save the model\n",
    "            model_dir = os.path.join(path, 'model')\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.model.to(torch.device('cpu'))\n",
    "            torch.save(model.model, os.path.join(model_dir, f\"{fname}.pth\"))\n",
    "            logger.info(f\"Saved model to {model_dir}\")\n",
    "\n",
    "        if cv_results is None:\n",
    "            logger.warning(\"Won't save any CV results, since it's not provided\")\n",
    "        else:\n",
    "            # Save cross-validation results\n",
    "            results_dir = os.path.join(path, 'results')\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            results_file = os.path.join(results_dir, f\"{fname}_cv.csv\")\n",
    "            pd.DataFrame(cv_results).to_csv(results_file)\n",
    "            logger.info(f\"Saved CV results to {results_file}\")\n",
    "\n",
    "    def _check_modelling_prerequs(self, pipeline_steps):\n",
    "        # Check requirements\n",
    "        err = False\n",
    "        mes = \"\"\n",
    "        if self.X is None or self.y is None:\n",
    "            mes = mes + \"1) Please call prepare_data() with your preferred config or set X, y, and groups\"\n",
    "            err = True\n",
    "        if not any('model' in tup for tup in pipeline_steps):\n",
    "            mes = mes + \"2) Caution! Your pipeline must include a step named 'model' for the model\"\n",
    "            err = True\n",
    "        return err, mes\n",
    "\n",
    "    def _get_config_vals(self, config):\n",
    "        # Extract config values\n",
    "        if config.get(\"params_cv\", None) is None:\n",
    "            logger.warning(\"No param grid for (nested) resampling detected - will fit model with default HPs on complete data\")\n",
    "            return None, False, False, False\n",
    "        if config.get('monitor', None) is None:\n",
    "            logger.info(\"No additional monitoring detected\")\n",
    "        return config['params_cv'], config.get('monitor', None), config.get('do_nested_resampling', True), config.get('refit', True)\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # Set parameters\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def _set_seed(self, seed=1234):\n",
    "        # Set random seeds\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        global random_state\n",
    "        random_state = check_random_state(seed)\n",
    "\n"
   ],
   "metadata": {
    "id": "xvLervBhRHrJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 4\n",
    "# Define hyperparameter grid for the nested resampling. To do so, params_cv can be adapted.\n",
    "# refit and do_nested_resampling should be true\n",
    "# fname_cv is the name by which the the results are stored in a csv file ->adapt\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'params_cv'  : {\n",
    "        # Define the number and size of hidden layers\n",
    "        'model__hidden_layers': [[512, 128],[512, 256], [256, 128], [1024], [512], [256], [128], [64], [32], [16]],\n",
    "        # Learning rate for optimization\n",
    "        'model__learning_rate': [0.00001, 0.0001],\n",
    "        # Batch size for training\n",
    "        'model__batch_size': [64],\n",
    "        # Number of training epochs\n",
    "        'model__num_epochs': [500],\n",
    "        # Dropout rate for regularization\n",
    "        'model__dropout': [0.2, 0.4],\n",
    "        'model__device': ['cuda']\n",
    "    },\n",
    "    'refit': True,\n",
    "    'do_nested_resampling': True,\n",
    "    'path' : \"\",\n",
    "    'fname_cv' : 'result_intersection'\n",
    "}\n",
    "\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "\n",
    "ds_pipeline_steps = [\n",
    "    ('model', DeepSurvModel())\n",
    "]\n",
    "\n",
    "nstd_res_result, cmplt_model, cmplt_pipeline = mp.do_modelling(ds_pipeline_steps, MODEL_CONFIG)\n",
    "\n",
    "print(nstd_res_result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqBMn9PORK5I",
    "outputId": "ed1acd89-a5ed-4bf1-f6cf-45f928d3df16"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 45\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   7.3s\n",
      "Early stopping at epoch 31\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.7s\n",
      "Early stopping at epoch 82\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   3.0s\n",
      "Early stopping at epoch 34\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 63\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.2s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 17\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 45\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 43\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 48\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.8s\n",
      "Early stopping at epoch 39\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 48\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 41\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 39\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 31\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 22\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 31\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 43\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.6s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 36\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.7s\n",
      "Early stopping at epoch 71\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.6s\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 30\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 48\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 28\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 79\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   3.1s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 30\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 82\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.8s\n",
      "Early stopping at epoch 39\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 28\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 43\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.7s\n",
      "Early stopping at epoch 26\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 40\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 32\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 34\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 48\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 36\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 79\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.8s\n",
      "Early stopping at epoch 21\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 26\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 37\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 27\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 63\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.2s\n",
      "Early stopping at epoch 41\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 71\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   2.6s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 43\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.8s\n",
      "Early stopping at epoch 26\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 23\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.8s\n",
      "Early stopping at epoch 39\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.1s\n",
      "Early stopping at epoch 38\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 26\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 37\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 18\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 27\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 20\n",
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 31\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.2s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 40\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 19\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 27\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any model, since it's not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 35\n",
      "Fitting 9 folds for each of 1 candidates, totalling 9 fits\n",
      "Early stopping at epoch 17\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.6s\n",
      "Early stopping at epoch 22\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 30\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.3s\n",
      "Early stopping at epoch 30\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.5s\n",
      "Early stopping at epoch 32\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n",
      "Early stopping at epoch 27\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.0s\n",
      "Early stopping at epoch 23\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.9s\n",
      "Early stopping at epoch 20\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   0.7s\n",
      "Early stopping at epoch 35\n",
      "[CV] END model__batch_size=64, model__device=cuda, model__dropout=0.2, model__hidden_layers=[16], model__learning_rate=0.0001, model__num_epochs=500; total time=   1.4s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:__main__:Won't save any CV results, since it's not provided\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 52\n",
      "{'mean_score': 0.6299091452259782, 'std_score': 0.07269191686464684, 'fold_results': [{'test_cohort': 'Atlanta_2014_Long', 'test_score': 0.5, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([2.25614724]), 'std_fit_time': array([2.03439166]), 'mean_score_time': array([0.00177008]), 'std_score_time': array([0.00069444]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.59127064]), 'split1_test_score': array([0.54677755]), 'split2_test_score': array([0.70131537]), 'split3_test_score': array([0.64073072]), 'split4_test_score': array([0.66968808]), 'split5_test_score': array([0.68386023]), 'split6_test_score': array([0.69836957]), 'split7_test_score': array([0.74940938]), 'mean_test_score': array([0.66017769]), 'std_test_score': array([0.06098206]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Belfast_2018_Jain', 'test_score': 0.5793898509867096, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.27706927]), 'std_fit_time': array([0.2544409]), 'mean_score_time': array([0.00150743]), 'std_score_time': array([0.00026817]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.52254335]), 'split1_test_score': array([0.55197505]), 'split2_test_score': array([0.70388194]), 'split3_test_score': array([0.63599459]), 'split4_test_score': array([0.68408424]), 'split5_test_score': array([0.64642263]), 'split6_test_score': array([0.74534161]), 'split7_test_score': array([0.73894701]), 'mean_test_score': array([0.6536488]), 'std_test_score': array([0.0766674]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPC_GENE_2017_Fraser', 'test_score': 0.5488565488565489, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.43616927]), 'std_fit_time': array([0.52134421]), 'mean_score_time': array([0.00168264]), 'std_score_time': array([0.00036448]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.53121387]), 'split1_test_score': array([0.59388844]), 'split2_test_score': array([0.68206609]), 'split3_test_score': array([0.62043302]), 'split4_test_score': array([0.6686217]), 'split5_test_score': array([0.69384359]), 'split6_test_score': array([0.73990683]), 'split7_test_score': array([0.73860952]), 'mean_test_score': array([0.65857288]), 'std_test_score': array([0.06780215]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CPGEA_2020_Li', 'test_score': 0.6916907282643567, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.28878394]), 'std_fit_time': array([0.74474503]), 'mean_score_time': array([0.00161019]), 'std_score_time': array([0.00021036]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.54450867]), 'split1_test_score': array([0.59026379]), 'split2_test_score': array([0.53118503]), 'split3_test_score': array([0.61028417]), 'split4_test_score': array([0.66382298]), 'split5_test_score': array([0.60898502]), 'split6_test_score': array([0.74262422]), 'split7_test_score': array([0.7480594]), 'mean_test_score': array([0.62996666]), 'std_test_score': array([0.07685107]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CamCap_2016_Ross_Adams', 'test_score': 0.6231393775372125, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.42095962]), 'std_fit_time': array([0.58058811]), 'mean_score_time': array([0.00160083]), 'std_score_time': array([0.00035861]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.58034682]), 'split1_test_score': array([0.59278091]), 'split2_test_score': array([0.56237006]), 'split3_test_score': array([0.68591594]), 'split4_test_score': array([0.6627566]), 'split5_test_score': array([0.64808652]), 'split6_test_score': array([0.70923913]), 'split7_test_score': array([0.73827202]), 'mean_test_score': array([0.647471]), 'std_test_score': array([0.0596982]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'CancerMap_2017_Luca', 'test_score': 0.6691548920287923, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.30728853]), 'std_fit_time': array([0.58583783]), 'mean_score_time': array([0.00181493]), 'std_score_time': array([0.00066181]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.52803468]), 'split1_test_score': array([0.59499597]), 'split2_test_score': array([0.56029106]), 'split3_test_score': array([0.72922682]), 'split4_test_score': array([0.60013532]), 'split5_test_score': array([0.62312812]), 'split6_test_score': array([0.74728261]), 'split7_test_score': array([0.7480594]), 'mean_test_score': array([0.64139425]), 'std_test_score': array([0.08209844]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'DKFZ_2018_Gerhauser', 'test_score': 0.6231281198003328, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.36872676]), 'std_fit_time': array([0.70970812]), 'mean_score_time': array([0.00159505]), 'std_score_time': array([0.00031736]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.56965318]), 'split1_test_score': array([0.59237817]), 'split2_test_score': array([0.58835759]), 'split3_test_score': array([0.67597048]), 'split4_test_score': array([0.63328823]), 'split5_test_score': array([0.66835511]), 'split6_test_score': array([0.6867236]), 'split7_test_score': array([0.74940938]), 'mean_test_score': array([0.64551697]), 'std_test_score': array([0.056939]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'MSKCC_2010_Taylor', 'test_score': 0.6948757763975155, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([0.97961968]), 'std_fit_time': array([0.18946783]), 'mean_score_time': array([0.00161779]), 'std_score_time': array([0.00028683]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.51184971]), 'split1_test_score': array([0.58976037]), 'split2_test_score': array([0.55093555]), 'split3_test_score': array([0.70067372]), 'split4_test_score': array([0.61434371]), 'split5_test_score': array([0.66782191]), 'split6_test_score': array([0.60648918]), 'split7_test_score': array([0.74535943]), 'mean_test_score': array([0.6234042]), 'std_test_score': array([0.07246794]), 'rank_test_score': array([1], dtype=int32)}}, {'test_cohort': 'Stockholm_2016_Ross_Adams', 'test_score': 0.7389470131623355, 'best_params': {'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}, 'inner_cv_results': {'mean_fit_time': array([1.01046845]), 'std_fit_time': array([0.24210744]), 'mean_score_time': array([0.00145546]), 'std_score_time': array([0.00037358]), 'param_model__batch_size': masked_array(data=[64],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'param_model__device': masked_array(data=['cuda'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__dropout': masked_array(data=[0.2],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__hidden_layers': masked_array(data=[list([16])],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_model__learning_rate': masked_array(data=[0.0001],\n",
      "             mask=[False],\n",
      "       fill_value=1e+20), 'param_model__num_epochs': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value=999999), 'params': [{'model__batch_size': 64, 'model__device': 'cuda', 'model__dropout': 0.2, 'model__hidden_layers': [16], 'model__learning_rate': 0.0001, 'model__num_epochs': 500}], 'split0_test_score': array([0.5066474]), 'split1_test_score': array([0.58593435]), 'split2_test_score': array([0.55093555]), 'split3_test_score': array([0.67564966]), 'split4_test_score': array([0.62110961]), 'split5_test_score': array([0.65769128]), 'split6_test_score': array([0.60399334]), 'split7_test_score': array([0.71273292]), 'mean_test_score': array([0.61433677]), 'std_test_score': array([0.06314364]), 'rank_test_score': array([1], dtype=int32)}}]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 5\n",
    "\n",
    "\n",
    "# define params for model training\n",
    "model_params = {\n",
    "    'hidden_layers': [256, 128],\n",
    "    'learning_rate': 0.00001,\n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 500,\n",
    "    'dropout': 0.2,\n",
    "    'device': 'cuda',\n",
    "    'random_state': 123\n",
    "}\n",
    "\n",
    "mp = ModellingProcess()\n",
    "mp.prepare_data()\n",
    "# Train model fix given params\n",
    "model_to_save = DeepSurvModel(**model_params)\n",
    "\n",
    "\n",
    "model_to_save.fit(mp.X.values, mp.y)\n",
    "\n",
    "preds_train = model_to_save.predict(mp.X.values)\n",
    "\n",
    "save_dir = \"/content/my_saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_file = os.path.join(save_dir, \"deep_surv_model_pData.pkl\")\n",
    "\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model_to_save, f)\n",
    "\n"
   ],
   "metadata": {
    "id": "hAIYrbF3ROEh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3b727946-a087-477f-99d4-9e70bfa431d8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Early stopping at epoch 53\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### Chunk 6\n",
    "\n",
    "# Load model\n",
    "model_file = \"/content/deep_surv_model.pkl\"\n",
    "with open(model_file, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(\"Modell erfolgreich geladen\")\n",
    "\n",
    "# Load test data\n",
    "test_pdata = pd.read_csv('/content/example_exprs.csv', index_col=0)\n",
    "\n",
    "# Prepare Features\n",
    "# Numerische Features\n",
    "numeric_features = ['AGE', 'GLEASON_SCORE', 'PRE_OPERATIVE_PSA']\n",
    "clinical_numeric = test_pdata[numeric_features]\n",
    "\n",
    "# Manually create one-hot encoding\n",
    "clinical_categorical = pd.DataFrame(index=test_pdata.index)\n",
    "clinical_categorical['TISSUE_Fresh_frozen'] = 1\n",
    "clinical_categorical['TISSUE_Snap_frozen'] = 0\n",
    "\n",
    "# Merge all features\n",
    "X_test = pd.concat([clinical_numeric, clinical_categorical], axis=1)\n",
    "\n",
    "print(\"Test Data Shape:\", X_test.shape)\n",
    "\n",
    "# Create survival object\n",
    "test_status = test_pdata['BCR_STATUS'].astype(bool).values\n",
    "test_time = test_pdata['MONTH_TO_BCR'].astype(float).values\n",
    "y_test = Surv.from_arrays(\n",
    "    event=test_status,\n",
    "    time=test_time,\n",
    "    name_event='status',\n",
    "    name_time='time'\n",
    ")\n",
    "\n",
    "X_test_array = X_test.values\n",
    "\n",
    "# Predicctions on test data\n",
    "test_predictions = loaded_model.predict(X_test_array)\n",
    "\n",
    "\n",
    "# Calculate c-index\n",
    "test_cindex = loaded_model.c_index(-test_predictions, y_test)\n",
    "print(\"\\nC-index auf Testdaten:\", test_cindex)\n",
    "\n",
    "# save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'sample_id': X_test.index,\n",
    "    'risk_score': test_predictions\n",
    "})\n",
    "results_df.to_csv('/content/test_predictions_clinical.csv')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqZGHajJTNs0",
    "outputId": "6de5bd74-e3bc-4de2-eb15-2f3969a6a790"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modell erfolgreich geladen\n",
      "Test Data Shape: (1091, 5)\n",
      "\n",
      "C-index auf Testdaten: 0.6818838670191515\n"
     ]
    }
   ]
  }
 ]
}
